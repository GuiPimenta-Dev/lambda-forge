{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"examples/blog/","title":"A Serverless Blog Application using AWS Cognito, Dynamo DB and S3","text":"<p>Coming soon...</p>"},{"location":"examples/guess-the-number/","title":"Creating a Guess the Number Game with DynamoDB","text":"<p>In this section, we will develop a \"Guess the Number\" game. Players will attempt to identify a randomly generated number by making successive guesses.</p> <p>The architecture of the Lambda functions we are going to create will be as follows:</p> <p> </p>"},{"location":"examples/guess-the-number/#configuring-dynamodb-tables-for-each-deployment-stage","title":"Configuring DynamoDB Tables for Each Deployment Stage","text":"<p>To ensure our application can operate smoothly across different environments, we'll create three separate DynamoDB tables on AWS DynamoDB console, each tailored for a distinct deployment stage: <code>Dev-Numbers</code>, <code>Staging-Numbers</code> and <code>Prod-Numbers</code>.</p> <p>Note</p> <p>Throughout this tutorial, we'll utilize PK as the Partition Key for all of our DynamoDB tables.</p> <p>Having acquired the ARNs for each stage-specific table, our next step involves integrating these ARNs into the <code>cdk.json</code> file. This crucial configuration enables our Cloud Development Kit (CDK) setup to correctly reference the DynamoDB tables according to the deployment stage.</p> <p>Here's how to update your <code>cdk.json</code> file to include the DynamoDB table ARNs for development, staging, and production environments:</p> cdk.json<pre><code>    \"dev\": {\n      \"arns\": {\n        \"numbers_table\": \"$DEV-NUMBERS-TABLE-ARN\"\n      }\n    },\n    \"staging\": {\n      \"arns\": {\n        \"numbers_table\": \"$STAGING-NUMBERS-TABLE-ARN\"\n      }\n    },\n    \"prod\": {\n      \"arns\": {\n        \"numbers_table\": \"$PROD-NUMBERS-TABLE-ARN\"\n      }\n    }\n</code></pre>"},{"location":"examples/guess-the-number/#incorporating-dynamodb-into-the-service-class","title":"Incorporating DynamoDB Into the Service Class","text":"<p>The subsequent phase in enhancing our application involves integrating the DynamoDB service within our service layer, enabling direct communication with DynamoDB tables. To accomplish this, utilize the following command:</p> <p><code>forge service dynamodb</code></p> <p>This command creates a new service file named <code>dynamodb.py</code> within the <code>infra/services</code> directory.</p> <pre><code>infra\n\u251c\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u2514\u2500\u2500 dynamodb.py\n</code></pre> <p>Below is the updated structure of our Service class, now including the DynamoDB service, demonstrating the integration's completion:</p> infra/services/__init__.py<pre><code>from infra.services.dynamodb import DynamoDB\nfrom infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\nfrom infra.services.layers import Layers\n\n\nclass Services:\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n        self.dynamodb = DynamoDB(scope, context)\n</code></pre> <p>Here is the newly established DynamoDB class:</p> infra/services/dynamodb.py<pre><code>from aws_cdk import aws_dynamodb as dynamodb\nfrom aws_cdk import aws_lambda as lambda_\nfrom aws_cdk import aws_lambda_event_sources as event_source\n\nfrom lambda_forge.trackers import invoke, trigger\n\n\nclass DynamoDB:\n    def __init__(self, scope, context) -&gt; None:\n\n        # self.dynamo = dynamodb.Table.from_table_arn(\n        #     scope,\n        #     \"Dynamo\",\n        #     context.resources[\"arns\"][\"dynamo_arn\"],\n        # )\n        ...\n\n    @trigger(service=\"dynamodb\", trigger=\"table\", function=\"function\")\n    def create_trigger(self, table: str, function: lambda_.Function) -&gt; None:\n        table_instance = getattr(self, table)\n        dynamo_event_stream = event_source.DynamoEventSource(\n            table_instance, starting_position=lambda_.StartingPosition.TRIM_HORIZON\n        )\n        function.add_event_source(dynamo_event_stream)\n\n    @invoke(service=\"dynamodb\", resource=\"table\", function=\"function\")\n    def grant_write(self, table: str, function: lambda_.Function) -&gt; None:\n        table_instance = getattr(self, table)\n        table_instance.grant_write_data(function)\n</code></pre> <p>Forge has already laid the groundwork by providing a commented code that outlines the structure for creating a DynamoDB table and retrieving its ARN from the <code>cdk.json</code> file. Additionally, it's worth noting that the DynamoDB class includes a specialized helper method aimed at streamlining the task of assigning query permissions.</p> <p>Note</p> <p>In this tutorial, we'll manually create AWS resources using the AWS console and directly insert the ARNs into our resource classes to reduce code clutter and simplify understanding. However, feel free to create your AWS resources directly using CDK in the corresponding classes if you prefer.</p> <p>Let's refine the class variables to directly reference our Numbers table.</p> infra/services/dynamodb.py<pre><code>class DynamoDB:\n    def __init__(self, scope, context: dict) -&gt; None:\n\n        self.numbers_table = dynamodb.Table.from_table_arn(\n            scope,\n            \"NumbersTable\",\n            context.resources[\"arns\"][\"numbers_table\"],\n        )\n</code></pre> <p>The <code>context.resources</code> object on line 11 contains only the resources that are pertinent to the current stage. By tapping into this, we can dynamically tweak our AWS resources according to the specific stage we're operating in.</p>"},{"location":"examples/guess-the-number/#creating-a-new-game","title":"Creating a New Game","text":"<p>Now that we have configured the DynamoDB tables, it's time to establish a function for creating new games.</p> <pre><code>forge function create_game --method \"POST\" --description \"Create a new game\" --belongs-to guess_the_number --no-tests --endpoint \"/games\" --public\n</code></pre> <p>This command initiates the creation of a new function named <code>create_game</code> within the <code>guess_the_number</code> folder. It specifies that the function will handle <code>POST</code> requests and sets the endpoint to <code>/games</code>. The function is designated as public, meaning it can be accessed by anyone with the URL.</p> <pre><code>functions\n\u2514\u2500\u2500 guess_the_number\n    \u2514\u2500\u2500 create_game\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>To create a new game in the database, the process involves receiving a minimum and a maximum number from the user. These values define the range within which a random number will be generated. Subsequently, a unique identifier (UUID) is assigned to the game. The generated random number, along with the UUID, is then saved to the DynamoDB table.</p> <p>The implementation of this process is outlined below:</p> functions/guess_the_number/create_game/main.py<pre><code>import json\nimport os\nimport random\nimport uuid\nfrom dataclasses import dataclass\n\nimport boto3\n\n\n@dataclass\nclass Input:\n    min_number: int\n    max_number: int\n\n\n@dataclass\nclass Output:\n    game_id: str\n\n\ndef lambda_handler(event, context):\n    # Initialize a DynamoDB resource using the boto3 library\n    dynamodb = boto3.resource(\"dynamodb\")\n    # Retrieve the DynamoDB table name from environment variables\n    NUMBERS_TABLE_NAME = os.environ.get(\"NUMBERS_TABLE_NAME\")\n    numbers_table = dynamodb.Table(NUMBERS_TABLE_NAME)\n\n    body = json.loads(event[\"body\"])\n\n    # Get the min and max number from the body\n    min_number = body.get(\"min_number\", 1)\n    max_number = body.get(\"max_number\", 100)\n\n    # Validate that the initial number is less than the end number\n    if min_number &gt;= max_number:\n        return {\"statusCode\": 400, \"body\": json.dumps({\"message\": \"min_number must be less than max_number\"})}\n\n    # Generate a unique game ID using uuid\n    game_id = str(uuid.uuid4())\n    # Generate a random number between the initial and end numbers\n    random_number = random.randint(min_number, max_number)\n\n    # Store the game ID and the random number in DynamoDB\n    numbers_table.put_item(\n        Item={\n            \"PK\": game_id,\n            \"number\": random_number,\n        }\n    )\n\n    return {\"statusCode\": 200, \"body\": json.dumps({\"game_id\": game_id})}\n</code></pre> <p>Next, we need to configure the function to integrate it with the DynamoDB table and set up the appropriate environment variables for accurate table selection.</p> functions/guess_the_number/create_game/config.py<pre><code>from infra.services import Services\n\n\nclass CreateGameConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"CreateGame\",\n            path=\"./functions/guess_the_number\",\n            description=\"Creates a new guess the number game\",\n            directory=\"create_game\",\n            environment={\n              \"NUMBERS_TABLE_NAME\": services.dynamodb.numbers_table.table_name\n            },\n        )\n\n        services.api_gateway.create_endpoint(\"POST\", \"/games\", function, public=True)\n\n        services.dynamodb.grant_write(\"numbers_table\", function)\n</code></pre>"},{"location":"examples/guess-the-number/#making-a-guess","title":"Making a Guess","text":"<p>Now that the game is set up in our table, we can begin the guessing phase. Depending on the user's input, the system will respond with <code>correct</code>, <code>higher</code> or <code>lower</code> to guide the user on how their guess compares to the actual number.</p> <pre><code>forge function make_guess --method \"GET\" --description \"Make a guess for a particular game\" --belongs-to guess_the_number --no-tests --endpoint \"/games/{game_id}\" --public\n</code></pre> <p>This command establishes a new function called <code>make_guess</code> in the <code>guess_the_number</code> folder. It is configured to handle <code>GET</code> requests and utilizes the endpoint <code>/games/{game_id}</code>, which requires the game ID to be included in the URL. The function is marked as public, allowing anyone with the URL to access it.</p> <pre><code>functions\n\u2514\u2500\u2500 guess_the_number\n    \u251c\u2500\u2500 create_game\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2514\u2500\u2500 main.py\n    \u2514\u2500\u2500 make_guess\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>To implement the guess function, we need to receive a user's guess as a query parameter and compare it with the actual number stored in the database. Based on this comparison, we will return the appropriate response to guide the user.</p> functions/guess_the_number/make_guess/main.py<pre><code>import json\nimport os\nfrom dataclasses import dataclass\n\nimport boto3\n\n\n@dataclass\nclass Path:\n    game_id: str\n\n\n@dataclass\nclass Input:\n    guess: int\n\n\n@dataclass\nclass Output:\n    answer: str\n\n\n# Main handler function for the Lambda to process incoming requests\ndef lambda_handler(event, context):\n    # Initialize a DynamoDB resource using boto3 and get the table name from environment variables\n    dynamodb = boto3.resource(\"dynamodb\")\n    NUMBERS_TABLE_NAME = os.environ.get(\"NUMBERS_TABLE_NAME\")\n    numbers_table = dynamodb.Table(NUMBERS_TABLE_NAME)\n\n    # Extract the game_id from path parameters in the event object\n    game_id = event[\"pathParameters\"][\"game_id\"]\n    # Extract the guess number from query string parameters in the event object\n    guess = event[\"queryStringParameters\"][\"guess\"]\n\n    # Retrieve the item from DynamoDB based on the game_id\n    response = numbers_table.get_item(Key={\"PK\": game_id})\n    # Extract the stored random number from the response\n    random_number = int(response[\"Item\"][\"number\"])\n\n    # Compare the guess to the random number and prepare the answer\n    if int(guess) == random_number:\n        answer = \"correct\"\n    elif int(guess) &lt; random_number:\n        answer = \"higher\"\n    else:\n        answer = \"lower\"\n\n    return {\"statusCode\": 200, \"body\": json.dumps({\"answer\": answer})}\n</code></pre> <p>Now, let's configure the necessary settings to enable data retrieval from the numbers table in DynamoDB.</p> functions/guess_the_number/make_guess/config.py<pre><code>from infra.services import Services\n\n\nclass MakeGuessConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"MakeGuess\",\n            path=\"./functions/guess_the_number\",\n            description=\"Make a guess for a particular game\",\n            directory=\"make_guess\",\n            environment={\n                \"NUMBERS_TABLE_NAME\": services.dynamodb.numbers_table.table_name\n            },\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/games/{game_id}\", function, public=True)\n\n        services.dynamodb.numbers_table.grant_read_data(function)\n</code></pre>"},{"location":"examples/guess-the-number/#deploying-the-functions","title":"Deploying the Functions","text":"<p>Next, we'll commit our code and push it to GitHub, following these steps:</p> <pre><code># Send your changes to stage\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"Guess The Number Game\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>This workflow ensures that our code moves sequentially through the development, staging, and production environments, triggering our three distinct deployment pipelines.</p> <p></p> <p>Upon completion of these pipelines, the Guess The Number game is deployed and available across all environments.</p>"},{"location":"examples/guess-the-number/#testing-the-functions","title":"Testing The Functions","text":"<p>We've deployed our function across three distinct environments. For simplicity, this document will focus on the <code>dev</code> environment, though the steps described are directly applicable to the other environments as well.</p>"},{"location":"examples/guess-the-number/#initiating-a-game-session","title":"Initiating a Game Session","text":"<p>To begin testing, we initiate a game session by sending a POST request to create a new game with a number range from 1 to 10.</p> <pre><code>curl --request POST \\\n  --url 'https://api.lambda-forge.com/dev/games' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"min_number\": 1,\n    \"max_number\": 10\n}'\n</code></pre> <p>This request generates the following response, including a unique Game ID:</p> <pre><code>{\n  \"game_id\": \"794eb9ec-79ae-4b56-9523-2fc8d38c341a\"\n}\n</code></pre>"},{"location":"examples/guess-the-number/#making-guesses","title":"Making Guesses","text":"<p>Now, with our game created and the ID acquired, we proceed by making guesses to find the correct number.</p>"},{"location":"examples/guess-the-number/#first-guess-1","title":"First Guess: 1","text":"<pre><code>curl --request GET \\\n  --url 'https://api.lambda-forge.com/dev/games/794eb9ec-79ae-4b56-9523-2fc8d38c341a?guess=1' \\\n</code></pre> <p>Response:</p> <pre><code>{\n  \"answer\": \"higher\"\n}\n</code></pre>"},{"location":"examples/guess-the-number/#second-guess-3","title":"Second Guess: 3","text":"<pre><code>curl --request GET \\\n  --url 'https://api.lambda-forge.com/dev/games/794eb9ec-79ae-4b56-9523-2fc8d38c341a?guess=3' \\\n</code></pre> <p>Response:</p> <pre><code>{\n  \"answer\": \"lower\"\n}\n</code></pre> <p>Given the responses, the correct number must be 2. Let's confirm by making the final guess.</p>"},{"location":"examples/guess-the-number/#final-guess-2","title":"Final Guess: 2","text":"<pre><code>curl --request GET \\\n  --url 'https://api.lambda-forge.com/dev/games/794eb9ec-79ae-4b56-9523-2fc8d38c341a?guess=2' \\\n</code></pre> <p>Response:</p> <pre><code>{\n  \"answer\": \"correct\"\n}\n</code></pre> <p>\ud83c\udf89 Success! The Guess The Number game is functioning perfectly across all environments, confirming the reliability and effectiveness of our deployment strategy.</p>"},{"location":"examples/image-to-qr-code-converter/","title":"Converting Image to QR Code with AWS S3, Secrets Manager and Email Notifications","text":"<p>In this part, we're going to cover how to make a function that turns images uploaded by users into QR codes. When a user sends a request, the image gets processed, saved on Amazon S3, and then sent to them via email so they can easily check out the results.</p> <p>The structure of the Lambda functions we plan to develop will be as follows:</p> <p> </p>"},{"location":"examples/image-to-qr-code-converter/#incorporating-s3-into-the-service-class","title":"Incorporating S3 Into the Service Class","text":"<p>Let's start creating three distinct buckets, each dedicated to a specific stage: <code>Dev-Lambda-Forge-Images</code>, <code>Staging-Lambda-Forge-Images</code> and <code>Prod-Lambda-Forge-Images</code>.</p> <p>Note</p> <p>Keep in mind that your bucket name must be unique across all AWS regions. Therefore, you'll need to select distinct names for your project.</p> <p>Now place the arns on your <code>cdk.json</code>.</p> cdk.json<pre><code>   \"dev\": {\n      \"base_url\": \"https://api.lambda-forge.com/dev\",\n      \"arns\": {\n        \"urls_table\": \"$DEV-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$DEV-IMAGES-BUCKET-ARN\"\n      }\n    },\n    \"staging\": {\n      \"base_url\": \"https://api.lambda-forge.com/staging\",\n      \"arns\": {\n        \"urls_table\": \"$STAGING-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$STAGING-IMAGES-BUCKET-ARN\"\n      }\n    },\n    \"prod\": {\n      \"base_url\": \"https://api.lambda-forge.com\",\n      \"arns\": {\n        \"urls_table\": \"$PROD-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$PROD-IMAGES-BUCKET-ARN\"\n      }\n    }\n</code></pre> <p>The next step involves integrating the S3 service into our service layer, facilitating direct communication with S3 buckets. To achieve this, execute the following command:</p> <p><code>forge service s3</code></p> <p>This command generates a new service file named <code>s3.py</code> within the infra/services directory, as illustrated below:</p> <pre><code>infra\n\u2514\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u251c\u2500\u2500 dynamodb.py\n    \u251c\u2500\u2500 layers.py\n    \u2514\u2500\u2500 s3.py\n</code></pre> <p>Below showcases the updated structure of our Service class, now incorporating the S3 service, indicating the successful integration:</p> infra/services/__init__.py<pre><code>from infra.services.s3 import S3\nfrom infra.services.dynamodb import DynamoDB\nfrom infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\nfrom infra.services.layers import Layers\n\n\nclass Services:\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n        self.layers = Layers(scope)\n        self.dynamodb = DynamoDB(scope, context)\n        self.s3 = S3(scope, context)\n</code></pre> <p>Here is the newly established S3 class:</p> infra/services/s3<pre><code>from aws_cdk import aws_s3 as s3\nfrom aws_cdk import aws_s3_notifications\nfrom lambda_forge.trackers import invoke, trigger\n\n\nclass S3:\n    def __init__(self, scope, context) -&gt; None:\n\n        # self.s3 = s3.Bucket.from_bucket_arn(\n        #     scope,\n        #     \"S3\",\n        #     bucket_arn=context.resources[\"arns\"][\"s3_arn\"],\n        # )\n        ...\n\n    @trigger(service=\"s3\", trigger=\"bucket\", function=\"function\")\n    def create_trigger(self, bucket, function, event=s3.EventType.OBJECT_CREATED):\n        bucket = getattr(self, bucket)\n        notifications = aws_s3_notifications.LambdaDestination(function)\n        bucket.add_event_notification(event, notifications)\n        bucket.grant_read(function)\n\n    @invoke(service=\"s3\", resource=\"bucket\", function=\"function\")\n    def grant_write(self, bucket, function):\n        bucket = getattr(self, bucket)\n        bucket.grant_write(function)\n</code></pre> <p>As seen, Forge has created the class with a helper method to streamline the creation of a trigger between a bucket and a lambda function.</p> <p>Let's update the class variables to directly reference our recently created bucket.</p> infra/services/s3.py<pre><code>class S3:\n    def __init__(self, scope, context: dict) -&gt; None:\n\n        self.images_bucket = s3.Bucket.from_bucket_arn(\n            scope,\n            \"ImagesBucket\",\n            bucket_arn=context.resources[\"arns\"][\"images_bucket\"],\n        )\n</code></pre> <p>Excellent! This approach configures our framework to utilize each ARN on its designated stage effectively.</p>"},{"location":"examples/image-to-qr-code-converter/#incorporating-secrets-manager-into-the-services-class","title":"Incorporating Secrets Manager into the Services Class","text":"<p>Since we are dealing with emails, we must use usernames and passowrd. Hardcoding email credentials directly into the code exposes them to potential breaches. To mitigate this risk, we'll implement a more secure approach using AWS Secrets Manager, a service designed to safeguard sensitive information such as secret keys.</p> <p>To create a new secrets manager service, simply type:</p> <pre><code>forge service secrets_manager\n</code></pre> <p>Similar to the S3 class, Forge will generate the new service file within the <code>infra/services</code> directory and seamlessly integrate it into the Services class.</p> <pre><code>infra\n\u2514\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u251c\u2500\u2500 dynamodb.py\n    \u251c\u2500\u2500 layers.py\n    \u251c\u2500\u2500 s3.py\n    \u2514\u2500\u2500 secrets_manager.py\n</code></pre> <p>Here's the newly established class:</p> infra/services/secrets_manager.py<pre><code>from aws_cdk import aws_secretsmanager as secrets_manager\n\n\nclass SecretsManager:\n    def __init__(self, scope, resources) -&gt; None:\n\n        # self.secrets_manager = secrets_manager.Secret.from_secret_complete_arn(\n        #     scope,\n        #     id=\"SecretsManager\",\n        #     secret_complete_arn=resources[\"arns\"][\"secrets_manager_arn\"],\n        # )\n        pass\n</code></pre> <p>Now, head over to the AWS Secrets Manager panel in the AWS console and create a new secret. Within this secret, store both the email address and an app password.</p> <p>Warning</p> <p>Note that you shouldn't save your regular GMAIL password; instead, use an app password. Refer to Sign in with app passwords to generate your app password.</p> <p>Now that we have the secret ARN in hand, let's proceed to update the Secrets Manager class accordingly.</p> infra/services/secrets_manager.py<pre><code>class SecretsManager:\n    def __init__(self, scope, resources) -&gt; None:\n\n        self.gmail_secret = secrets_manager.Secret.from_secret_complete_arn(\n            scope,\n            id=\"GmailSecret\",\n            secret_complete_arn=\"$GMAIL-SECRET-ARN\",\n        )\n</code></pre>"},{"location":"examples/image-to-qr-code-converter/#using-a-non-public-library-as-lambda-layer","title":"Using a Non-Public Library as Lambda Layer","text":"<p>To convert the image into a qr code, we are going to use an external library called <code>qrcode</code>. Unlike more popular layers, we're dealing with a library for which AWS doesn't provide a public layer.</p>"},{"location":"examples/image-to-qr-code-converter/#creating-an-external-library-in-lambda-forge","title":"Creating an External Library in Lambda Forge","text":"<p>To create an external library in Lambda Forge, follow these steps:</p> <ol> <li> <p>Run the following command: <pre><code>forge layer --external qrcode\n</code></pre></p> </li> <li> <p>Lambda Forge will automatically deploy this layer to AWS and print out the ARN of the layer.</p> </li> <li> <p>Once you have the ARN of the layer, paste it into the <code>Layers</code> class.</p> </li> </ol> infra/services/layers.py<pre><code>from aws_cdk import aws_lambda as _lambda\nfrom lambda_forge import Path\n\n\nclass Layers:\n    def __init__(self, scope) -&gt; None:\n\n        self.qrcode_layer = _lambda.LayerVersion.from_layer_version_arn(\n            scope,\n            id=\"QrCodeLayer\",\n            layer_version_arn=\"$QR-CODE-LAYER-ARN\",\n        )\n</code></pre> <p>It's essential to include both libraries in our <code>requirements.txt</code> file to ensure they are installed when deploying our application.</p> requirements.txt<pre><code>qrcode==7.4.2\n</code></pre>"},{"location":"examples/image-to-qr-code-converter/#implementing-the-function-to-convert-image-to-qr-code","title":"Implementing the Function to Convert Image to QR Code","text":"<p>With our layers now set up, it's time to create our new function.</p> <pre><code>forge function qrcode --method \"POST\" --description \"Converts an image into a qr code\" --belongs-to \"images\" --no-tests --public --endpoint \"images/qrcode\"\n</code></pre> <p>We now have the following directory:</p> <pre><code>functions\n\u2514\u2500\u2500 images\n    \u2514\u2500\u2500 qrcode\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>Let's dive into implementing this function, which will handle user input consisting of a <code>url</code> to convert the image parameter and an <code>email</code> parameter for sending notification.</p> functions/images/img_to_qrcode/main.py<pre><code>import hashlib\nimport json\nimport os\nfrom dataclasses import dataclass\nfrom io import BytesIO\n\nimport boto3\nimport qrcode\n\n\n@dataclass\nclass Input:\n    url: str\n    email: str\n\n\n@dataclass\nclass Output:\n    pass\n\n\ndef lambda_handler(event, context):\n\n    # Parse the input event to get the URL of the image and the S3 bucket name\n    body = json.loads(event[\"body\"])\n    url = body.get(\"url\")\n\n    # Retrieve the S3 bucket name from environment variables\n    bucket_name = os.environ.get(\"BUCKET_NAME\")\n\n    # Generate QR code from the image\n    qr = qrcode.QRCode()\n    qr.add_data(url)\n    qr.make()\n\n    # Create an image from the QR code\n    qr_image = qr.make_image()\n\n    # Convert the QR code image to bytes\n    qr_byte_arr = BytesIO()\n    qr_image.save(qr_byte_arr)\n    qr_byte_arr = qr_byte_arr.getvalue()\n\n    # Create the file name with a hash based on the input URL\n    file_name = f\"{hashlib.md5(url.encode()).hexdigest()}.jpg\"\n\n    # Initialize the S3 client\n    s3_client = boto3.client(\"s3\")\n\n    # Upload the QR code image to S3\n    s3_client.put_object(\n        Bucket=bucket_name,\n        Key=file_name,\n        Body=qr_byte_arr,\n        ContentType=\"image/png\",\n        Metadata={\"url\": url, \"email\": body.get(\"email\")},\n    )\n\n    return {\"statusCode\": 200}\n</code></pre> <p>Essentially, our function retrieves the URL from the parameters provided by the user. It then utilizes the qrcode library to convert the URL into a QR code before storing it in the S3 bucket. Additionally, the function saves the original url along with the associated email as metadata for future reference.</p> <p>Now, it's configuration.</p> functions/images/qrcode/config.py<pre><code>from infra.services import Services\n\n\nclass QrcodeConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Qrcode\",\n            path=\"./functions/images\",\n            description=\"Converts an image into a qr code\",\n            directory=\"qrcode\",\n            layers=[services.layers.qrcode_layer],\n            environment={\n                \"BUCKET_NAME\": services.s3.images_bucket.bucket_name,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\"POST\", \"/images/qrcode\", function, public=True)\n\n        services.s3.grant_write(\"images_bucket\", function)\n</code></pre>"},{"location":"examples/image-to-qr-code-converter/#implementing-the-mailer-function","title":"Implementing the Mailer Function","text":"<p>It's worth noting that in our previous implementation, we deliberately omitted email notifications. This exemplifies one of the advantages of serverless architecture: the ability to completely decouple functions from each other and initiate notifications through events.</p> <p>This is precisely the approach we're taking with the mailer function. Whenever a file is uploaded to the S3 bucket, an event will be triggered to run this Lambda function. With the assistance of metadata, the mailer Lambda function will be equipped with the necessary information to determine the appropriate email recipients for notifications.</p> <pre><code>forge function mailer --description \"Sends an email based on metadata when image enters the bucket\" --belongs-to \"images\" --no-api --no-tests\n</code></pre> <p>Here's how our updated directory looks now.</p> <pre><code>functions\n\u2514\u2500\u2500 images\n    \u251c\u2500\u2500 mailer\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2514\u2500\u2500 main.py\n    \u2514\u2500\u2500 qrcode\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>Let's whip up an eye-catching HTML layout to give our email a touch of elegance.</p> functions/images/mailer/template.html<pre><code>&lt;html&gt;\n    &lt;head&gt;\n        &lt;style&gt;\n            body {\n                font-family: Arial, sans-serif;\n                margin: 0;\n                padding: 0;\n                background-color: #f4f4f4;\n            }\n            .container {\n                background-color: #ffffff;\n                margin: 10px auto;\n                padding: 20px;\n                max-width: 600px;\n                border-radius: 8px;\n                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n            }\n            p {\n                font-size: 16px;\n                line-height: 1.5;\n                color: #555555;\n            }\n            .logo {\n                display: block;\n                margin: 0 auto 20px auto;\n                width: 100px;\n                height: auto;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"container\"&gt;\n            &lt;img\n                src=\"https://public-lambda-forge-logo.s3.us-east-2.amazonaws.com/wNSN2U7n9NiAKEItWlsrcdJ0RWFyZOmbNvsc6Kht84WsWVxuBz5O.png\"\n                alt=\"Lambda Forge Logo\"\n                class=\"logo\"\n            /&gt;\n            &lt;h1&gt;Your Image Is Ready!&lt;/h1&gt;\n            &lt;p&gt;Hello,&lt;/p&gt;\n            &lt;p&gt;\n                We're excited to let you know that your image has been processed and is\n                now attached to this email.\n            &lt;/p&gt;\n\n            &lt;p&gt;Please check the attachment to view it.&lt;/p&gt;\n\n            &lt;p&gt;\n                Made with \u2764\ufe0f by\n                &lt;b\n                    &gt;&lt;a\n                        href=\"https://docs.lambda-forge.com\"\n                        style=\"color: inherit; text-decoration: none;\"\n                        &gt;Lambda Forge&lt;/a\n                    &gt;&lt;/b\n                &gt;\n            &lt;/p&gt;\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Time to implement the mailer functionality!</p> functions/images/mailer/main.py<pre><code>import os\nimport smtplib\nfrom email.mime.application import MIMEApplication\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\nimport boto3\n\n\ndef lambda_handler(event, context):\n    # Initialize the S3 client\n    s3_client = boto3.client(\"s3\")\n\n    # Fetch the SMTP details from the environment variables\n    SMTP_HOST = os.environ[\"SMTP_HOST\"]\n    SMTP_PORT = os.environ[\"SMTP_PORT\"]\n    SMTP_USER = os.environ[\"SMTP_USER\"]\n    SMTP_PASS = os.environ[\"SMTP_PASS\"]\n\n    # Extract the bucket name and the object key from the event\n    record = event[\"Records\"][0]\n    bucket_name = record[\"s3\"][\"bucket\"][\"name\"]\n    object_key = record[\"s3\"][\"object\"][\"key\"]\n\n    # Fetch the image from S3\n    response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n\n    # Extract the receiver email from the metadata\n    receiver = response[\"Metadata\"][\"email\"]\n\n    # Create the multipart email\n    msg = MIMEMultipart()\n    sender_name = \"Lambda Forge\"\n\n    # Set the 'From' field, including both the name and the email:\n    msg[\"From\"] = f\"{sender_name} &lt;{SMTP_USER}&gt;\"\n    msg[\"To\"] = receiver\n    msg[\"Subject\"] = \"Image Processed Successfully!\"\n\n    # Join the current directory with the filename to get the full path of the HTML file\n    current_directory = os.path.dirname(os.path.abspath(__file__))\n    html_path = os.path.join(current_directory, \"template.html\")\n\n    # Read the HTML content\n    html = open(html_path).read()\n    msg.attach(MIMEText(html, \"html\"))\n\n    # Attach the image\n    image_data = response[\"Body\"].read()\n    file_name = object_key.split(\"/\")[-1]\n    part = MIMEApplication(image_data, Name=file_name)\n    part[\"Content-Disposition\"] = f'attachment; filename=\"{file_name}\"'\n    msg.attach(part)\n\n    # Send the email via Gmail's SMTP server, or use another server if not using Gmail\n    with smtplib.SMTP_SSL(SMTP_HOST, SMTP_PORT) as server:\n        server.login(SMTP_USER, SMTP_PASS)\n        server.sendmail(SMTP_USER, receiver, msg.as_string())\n</code></pre> <p>This function fetches essential email-sending details from environment variables such as <code>SMTP_HOST</code>, <code>SMTP_PORT</code>, <code>SMTP_USER</code>, and <code>SMTP_PASS</code>. It then retrieves the recipient's email address from the bucket's metadata and sends an email with the QR code attached.</p> <p>The elegance of this approach lies in its flexibility. We can incorporate multiple image processors, including tasks like image resizing, applying color filters, facial recognition, and more. None of these processors need to handle email sending directly. By simply saving the processed image inside the bucket, the corresponding functionality is seamlessly applied.</p> <p>Now, let's configure our Mailer function.</p> functions/images/mailer/config.py<pre><code>from infra.services import Services\n\n\nclass MailerConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Mailer\",\n            path=\"./functions/images\",\n            description=\"Sends an email when an image enters the bucket\",\n            directory=\"mailer\",\n            environment={\n                \"SMTP_HOST\": \"smtp.gmail.com\",\n                \"SMTP_PORT\": \"465\",\n                \"SMTP_USER\": services.secrets_manager.gmail_secret.secret_value_from_json(\"email\").unsafe_unwrap(),\n                \"SMTP_PASS\": services.secrets_manager.gmail_secret.secret_value_from_json(\"password\").unsafe_unwrap(),\n            },\n        )\n\n        services.s3.images_bucket.grant_read(function)\n        services.s3.create_trigger(\"images_bucket\", function)\n</code></pre> <p>With our existing setup, we configure the environment variables and grant read permissions to the function for accessing the bucket. Additionally, we utilize Forge's helper method to establish a trigger that activates when an object is created in the bucket, invoking the function.</p>"},{"location":"examples/image-to-qr-code-converter/#mitigating-security-risks-in-mailer-configuration","title":"Mitigating Security Risks in Mailer Configuration","text":"<p>Although the <code>/mailer/config.py</code> file may seem functional, its implementation poses a significant security risk. Hardcoding credentials directly into environment variables exposes them to potential breaches, as the secret will be visible on the Lambda Function panel.</p> <p></p> <p>To mitigate this risk, let's modify our <code>main.py</code> file slightly. Instead of retrieving the Gmail credentials from environment variables, we'll directly retrieve them from AWS Secrets Manager.</p> functions/images/mailer/main.py<pre><code>def lambda_handler(event, context):\n    # Initialize the S3 client\n    s3_client = boto3.client(\"s3\")\n\n    # Fetch the SMTP details from the environment variables\n    SMTP_HOST = os.environ[\"SMTP_HOST\"]\n    SMTP_PORT = os.environ[\"SMTP_PORT\"]\n\n    import json\n\n    # Initialize the Secrets Manager client\n    sm_client = boto3.client('secretsmanager')\n    secret_name = '$SECRET-NAME'\n\n    # Retrieve the secret value from Secrets Manager\n    response = sm_client.get_secret_value(SecretId=secret_name)\n    secret = json.loads(response['SecretString'])\n\n    # Extract SMTP credentials from the secret data\n    SMTP_USER = secret[\"email\"]\n    SMTP_PASS = secret[\"password\"]\n</code></pre> <p>That's quite a bit of boilerplate code for such a straightforward task! \ud83d\ude30 Considering the critical importance of security, we'll probably employ this code snippet in numerous functions.</p>"},{"location":"examples/image-to-qr-code-converter/#creating-a-custom-layer-to-avoid-code-duplication","title":"Creating a Custom Layer to Avoid Code Duplication","text":"<p>To avoid duplicating the previous code throughout our project, let's establish a new <code>sm_utils</code> custom layer. This approach will streamline the process, allowing all lambda functions that need to retrieve a secret from Secrets Manager to do so with just a single line of code.</p> <p>Check out AWS Lambda Development with Custom Layers to delve deeper into custom layers in Lambda development.</p> <p>To create the new custom layer, simply type:</p> <pre><code>forge layer --custom sm_utils\n</code></pre> <p>This command creates the following directory:</p> <pre><code>layers\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 sm_utils\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 sm_utils.py\n</code></pre> <p>Additionally, a new layer has been incorporated into the Layers class.</p> infra/services/layers<pre><code>        self.qrcode_layer = _lambda.LayerVersion.from_layer_version_arn(\n            scope,\n            id=\"QrCodeLayer\",\n            layer_version_arn=\"arn:aws:lambda:us-east-2:211125768252:layer:QRCode:1\",\n        )\n\n        self.sm_utils_layer = _lambda.LayerVersion(\n            scope,\n            id='SmUtilsLayer',\n            code=_lambda.Code.from_asset(Path.layer('layers/sm_utils')),\n            compatible_runtimes=[_lambda.Runtime.PYTHON_3_9],\n            description='',\n         )\n</code></pre> <p>Now, it's time to level up the sm_utils layer by introducing a <code>get_secret</code> function. This handy feature will be shared across all our Lambda functions, simplifying our codebase.</p> layers/sm_utils/sm_utils.py<pre><code>import json\n\nimport boto3\n\n\ndef get_secret(secret_name: str):\n\n    # Initialize the Secrets Manager client\n    sm_client = boto3.client(\"secretsmanager\")\n\n    # Retrieve the secret value from Secrets Manager\n    response = sm_client.get_secret_value(SecretId=secret_name)\n\n    # Handle scenarios where the secret is stored as plain text instead of JSON.\n    try:\n        secret = json.loads(response[\"SecretString\"])\n\n    except json.JSONDecodeError:\n        secret = response[\"SecretString\"]\n\n    return secret\n</code></pre>"},{"location":"examples/image-to-qr-code-converter/#refactoring-the-mailer-function-to-use-custom-layers","title":"Refactoring The Mailer Function to Use Custom Layers","text":"<p>Below is the updated <code>main.py</code> file, now leveraging the new <code>sm_utils</code> layer.</p> functions/images/mailer/main.py<pre><code>import os\nimport smtplib\nfrom email.mime.application import MIMEApplication\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\nimport boto3\nimport sm_utils\n\n\ndef lambda_handler(event, context):\n    # Initialize the S3 client\n    s3_client = boto3.client(\"s3\")\n\n    # Fetch the SMTP details from the environment variables\n    SMTP_HOST = os.environ[\"SMTP_HOST\"]\n    SMTP_PORT = os.environ[\"SMTP_PORT\"]\n\n    # Get the secret name from env variable\n    SECRET_NAME = os.environ[\"SECRET_NAME\"]\n\n    # Get the secret from sm_utils layer\n    secret = sm_utils.get_secret(SECRET_NAME)\n\n    SMTP_USER = secret[\"email\"]\n    SMTP_PASS = secret[\"password\"]\n\n    # Extract the bucket name and the object key from the event\n    record = event[\"Records\"][0]\n    bucket_name = record[\"s3\"][\"bucket\"][\"name\"]\n    object_key = record[\"s3\"][\"object\"][\"key\"]\n\n    # Fetch the image from S3\n    response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n\n    # Extract the receiver email from the metadata\n    receiver = response[\"Metadata\"][\"email\"]\n\n    # Create the multipart email\n    msg = MIMEMultipart()\n    sender_name = \"Lambda Forge\"\n\n    # Set the 'From' field, including both the name and the email:\n    msg[\"From\"] = f\"{sender_name} &lt;{SMTP_USER}&gt;\"\n    msg[\"To\"] = receiver\n    msg[\"Subject\"] = \"Image Processed Successfully!\"\n\n    # Join the current directory with the filename to get the full path of the HTML file\n    current_directory = os.path.dirname(os.path.abspath(__file__))\n    html_path = os.path.join(current_directory, \"template.html\")\n\n    # Read the HTML content\n    html = open(html_path).read()\n    msg.attach(MIMEText(html, \"html\"))\n\n    # Attach the image\n    image_data = response[\"Body\"].read()\n    file_name = object_key.split(\"/\")[-1]\n    part = MIMEApplication(image_data, Name=file_name)\n    part[\"Content-Disposition\"] = f'attachment; filename=\"{file_name}\"'\n    msg.attach(part)\n\n    # Send the email via Gmail's SMTP server, or use another server if not using Gmail\n    with smtplib.SMTP_SSL(SMTP_HOST, SMTP_PORT) as server:\n        server.login(SMTP_USER, SMTP_PASS)\n        server.sendmail(SMTP_USER, receiver, msg.as_string())\n</code></pre> <p>Now, let's adjust the configuration to accommodate the changes necessary for the function.</p> functions/images/mailer/config.py<pre><code>from infra.services import Services\n\n\nclass MailerConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Mailer\",\n            path=\"./functions/images\",\n            description=\"Sends an email when an image enters the bucket\",\n            directory=\"mailer\",\n            layers=[services.layers.sm_utils_layer],\n            environment={\n                \"SMTP_HOST\": \"smtp.gmail.com\",\n                \"SMTP_PORT\": \"465\",\n                \"SECRET_NAME\": services.secrets_manager.gmail_secret.secret_name,\n            },\n        )\n\n        services.s3.images_bucket.grant_read(function)\n        services.s3.create_trigger(\"images_bucket\", function)\n\n        services.secrets_manager.gmail_secret.grant_read(function)\n</code></pre>"},{"location":"examples/image-to-qr-code-converter/#deploying-the-functions","title":"Deploying The Functions","text":"<p>Next, we'll commit our code and push it to GitHub, following these steps:</p> <pre><code># Send your changes to stage\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"Image to QR code converter with result being sent by email\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>This process guarantees that our code transitions systematically through the development, staging, and production environments. It activates our three specialized deployment pipelines, as illustrated by the pipelines running in the accompanying image.</p> <p></p> <p>Following the successful execution of these pipelines, the Image to QR code feature becomes accessible across the development, staging, and production stages, ensuring a seamless deployment.</p>"},{"location":"examples/image-to-qr-code-converter/#testing-the-image-to-qr-code-conversion","title":"Testing the Image to QR Code Conversion","text":"<p>We'll walk through testing our Image to QR Code Converter, focusing on the production environment for this demonstration. The procedure remains consistent across development and staging environments, with the only difference being the specific endpoints used.</p> <p>To convert an image URL into a QR code, we execute the following POST request:</p> <pre><code>curl --request POST \\\n  --url https://api.lambda-forge.com/images/qrcode \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"url\": \"https://public-lambda-forge-logo.s3.us-east-2.amazonaws.com/wNSN2U7n9NiAKEItWlsrcdJ0RWFyZOmbNvsc6Kht84WsWVxuBz5O.png\",\n    \"email\": \"$EMAIL\"\n}'\n</code></pre> <p>Shortly after the request is made, an email is dispatched to the provided address. </p> <p>The email contains a QR code attachment, as seen in the illustration below:</p> <p> </p> <p>Upon scanning the QR code, the original image is displayed:</p> <p></p> <p>\ud83c\udf89 Success! The Image to QR Code Converter function is now fully deployed and operational in all environments. \ud83d\ude80\u2728</p>"},{"location":"examples/introduction/","title":"Introduction","text":"<p>In this guide, we'll take you on a journey through the development process with Lambda Forge, illustrating the progression of projects through a hands-on, step-by-step approach within a unified codebase. Our methodology employs an incremental build strategy, where each new feature enhances the foundation laid by preceding projects, ensuring a cohesive and scalable architecture without duplicating efforts.</p> <p>To keep our focus sharp on AWS resources and Lambda Forge architecture, we'll skip over the detailed discussion of unit and integration tests here. Our objective is to provide a streamlined and informative learning path, striking a balance between technical detail and approachability to keep you engaged without feeling overwhelmed.</p> <p>To enhance usability and the overall user experience, we've implemented a custom domain, <code>https://api.lambda-forge.com</code>, making our URLs succinct and memorable across various deployment stages:</p> <ul> <li>Dev - <code>https://api.lambda-forge.com/dev</code></li> <li>Staging - <code>https://api.lambda-forge.com/staging</code></li> <li>Prod - <code>https://api.lambda-forge.com</code></li> </ul> <p>With that being said, let's forge some Lambdas!</p> <pre><code>forge project lambda-forge-examples --repo-owner \"$GITHUB-OWNER\" --repo-name \"$GITHUB-REPO\" --bucket \"$S3-BUCKET\"\n</code></pre> <p>API Docs: https://api.lambda-forge.com/docs.</p> <p>Source code: https://github.com/GuiPimenta-Dev/lambda-forge/tree/master/docs/examples</p>"},{"location":"examples/jwt-authentication/","title":"Implementing a Serverless Authentication System with JWT, Dynamo DB, Secrets Manager and KMS","text":"<p>In this section, we will develop a serverless authentication system using JWT authentication. This system effectively transmits information from the client and authenticates users to gain access to endpoints containing private information.</p> <p>JWT authentication is a secure method for transmitting information between parties as a JSON object. To gain a deeper understanding of JWT tokens and their functionality, you can refer to the article JSON Web Tokens.</p> <p>The design for the Lambda functions we intend to create will look like this:</p> <p> </p>"},{"location":"examples/jwt-authentication/#setting-up-the-dynamodb-tables","title":"Setting Up the DynamoDB Tables","text":"<p>To get started, we must create tables to store user credentials securely. For maximum decoupling of environments, proceed to your AWS console and create three separate tables, each designated for a specific stage: <code>Dev-Auth</code>, <code>Staging-Auth</code> and <code>Prod-Auth</code>.</p> <p>Once you have obtained the ARNs for these tables, let's integrate them into the <code>cdk.json</code> file within the corresponding environment.</p> cdk.json<pre><code>   \"dev\": {\n      \"base_url\": \"https://api.lambda-forge.com/dev\",\n      \"arns\": {\n        \"urls_table\": \"$DEV-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$DEV-IMAGES-BUCKET-ARN\",\n        \"auth_table\": \"$DEV-AUTH-TABLE-ARN\"\n      }\n    },\n    \"staging\": {\n      \"base_url\": \"https://api.lambda-forge.com/staging\",\n      \"arns\": {\n        \"urls_table\": \"$STAGING-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$STAGING-IMAGES-BUCKET-ARN\",\n        \"auth_table\": \"$STAGING-AUTH-TABLE-ARN\"\n      }\n    },\n    \"prod\": {\n      \"base_url\": \"https://api.lambda-forge.com\",\n      \"arns\": {\n        \"urls_table\": \"$PROD-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$PROD-IMAGES-BUCKET-ARN\",\n        \"auth_table\": \"$PROD-AUTH-TABLE-ARN\"\n      }\n    }\n</code></pre> <p>Next, we'll create a new variable class within the DynamoDB class to reference our JWT tables.</p> infra/services/dynamodb.py<pre><code>        self.urls_table = dynamodb.Table.from_table_arn(\n            scope,\n            \"URLsTable\",\n            context.resources[\"arns\"][\"urls_table\"],\n        )\n\n        self.auth_table = dynamodb.Table.from_table_arn(\n            scope,\n            \"AuthTable\",\n            context.resources[\"arns\"][\"auth_table\"],\n        )\n</code></pre>"},{"location":"examples/jwt-authentication/#implementing-password-hashing-with-kms","title":"Implementing Password Hashing with KMS","text":"<p>As we're dealing with sensitive data such as passwords, storing them in plain text poses a significant security risk. To mitigate this risk, we'll utilize KMS (Key Management Service), an AWS resource designed for hashing passwords and other sensitive information.</p> <p>To create a new KMS service, execute the following command:</p> <pre><code>forge service kms\n</code></pre> <p>This command creates a new file within the <code>infra/services</code> directory specifically for managing KMS keys.</p> <pre><code>infra\n\u2514\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u251c\u2500\u2500 dynamodb.py\n    \u251c\u2500\u2500 kms.py\n    \u251c\u2500\u2500 layers.py\n    \u251c\u2500\u2500 s3.py\n    \u2514\u2500\u2500 secrets_manager.py\n</code></pre> <p>Next, navigate to your AWS KMS console on AWS and create a new key. Then, update the KMS class with the ARN of the newly generated key.</p> infra/services/kms.py<pre><code>from aws_cdk import aws_kms as kms\n\n\nclass KMS:\n    def __init__(self, scope, context) -&gt; None:\n\n        self.auth_key = kms.Key.from_key_arn(\n            scope,\n            \"AuthKey\",\n            key_arn=\"$AUTH-KEY-ARN\",\n        )\n</code></pre>"},{"location":"examples/jwt-authentication/#creating-a-jwt-secret-on-secrets-manager","title":"Creating a JWT Secret on Secrets Manager","text":"<p>To validate JWT tokens securely, a secret is essential. This secret, usually a random string, acts as a key for verifying whether the token was generated from a trusted source. It ensures that only authorized parties can generate and verify tokens, preventing unauthorized access to protected resources.</p> <p>By storing the secret securely, you safeguard the integrity and confidentiality of your authentication system, mitigating the risk of unauthorized access and data breaches. Having that said, navigate to AWS Secrets Manager, create a new secret and save your random string there.</p> <p>After obtaining the secret ARN from AWS Secrets Manager, integrate it into the Secrets Manager class.</p> infra/services/secrets_manager.py<pre><code>class SecretsManager:\n    def __init__(self, scope, resources) -&gt; None:\n\n        self.gmail_secret = secrets_manager.Secret.from_secret_complete_arn(\n            scope,\n            id=\"GmailSecret\",\n            secret_complete_arn=\"$GMAIL-SECRET-ARN\",\n        )\n\n        self.jwt_secret = secrets_manager.Secret.from_secret_complete_arn(\n            scope,\n            id=\"JwtSecret\",\n            secret_complete_arn=\"$JWT-SECRET-ARN\",\n        )\n</code></pre>"},{"location":"examples/jwt-authentication/#using-the-pyjwt-public-layer","title":"Using the PYJWT Public Layer","text":"<p>To hash our JWT tokens, we'll leverage the widely-used Python library called <code>pyjwt</code>. Due to its popularity, AWS conveniently offers it as a public layer, streamlining our authentication implementation.</p> <ul> <li>PYJWT: <code>arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-PyJWT:3</code></li> </ul> <p>Let's now create a new class variable refencing the pyjwt layer.</p> infra/services/layers.py<pre><code>        self.sm_utils_layer = _lambda.LayerVersion(\n            scope,\n            id='SmUtilsLayer',\n            code=_lambda.Code.from_asset(Path.layer('layers/sm_utils')),\n            compatible_runtimes=[_lambda.Runtime.PYTHON_3_9],\n            description='',\n         )\n\n        self.pyjwt_layer = _lambda.LayerVersion.from_layer_version_arn(\n            scope,\n            id=\"JWTLayer\",\n            layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-PyJWT:3\",\n        )\n</code></pre> <p>Don't forget to add the pyjwt layer in the <code>requirements.txt</code></p> requirements.txt<pre><code>jwt==1.3.1\n</code></pre>"},{"location":"examples/jwt-authentication/#implementing-the-signup-function","title":"Implementing the SignUp Function","text":"<p>Now that we have all the necessary components set up, it's time to develop our authentication logic. We'll begin with the signup function, which is responsible for receiving an email and a password from the user. This function will then store them in the database, ensuring that the user is unique and storing a hashed version of the password for security purposes.</p> <pre><code>forge function signup --method \"POST\" --description \"Securely handle user registration with unique credentials.\" --public --belongs-to auth --no-tests --endpoint signup\n</code></pre> <p>This command generates a new function within the <code>auth</code> directory.</p> <pre><code>functions\n\u2514\u2500\u2500 auth\n    \u2514\u2500\u2500 signup\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>The signup functionality can be implemented as follows:</p> functions/auth/signup/main.py<pre><code>import json\nimport os\nfrom dataclasses import dataclass\n\nimport boto3\n\n\n@dataclass\nclass Input:\n    email: str\n    password: int\n\n\n@dataclass\nclass Output:\n    pass\n\n\ndef encrypt_with_kms(plaintext: str, kms_key_id: str) -&gt; str:\n    kms_client = boto3.client(\"kms\")\n    response = kms_client.encrypt(KeyId=kms_key_id, Plaintext=plaintext.encode())\n    return response[\"CiphertextBlob\"]\n\n\ndef lambda_handler(event, context):\n    # Retrieve the DynamoDB table name and KMS key ID from environment variables.\n    AUTH_TABLE_NAME = os.environ.get(\"AUTH_TABLE_NAME\")\n    KMS_KEY_ID = os.environ.get(\"KMS_KEY_ID\")\n\n    # Initialize a DynamoDB resource.\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table.\n    auth_table = dynamodb.Table(AUTH_TABLE_NAME)\n\n    # Parse the request body to get user data.\n    body = json.loads(event[\"body\"])\n\n    # Verify if the user already exists.\n    user = auth_table.get_item(Key={\"PK\": body[\"email\"]})\n    if user.get(\"Item\"):\n        return {\n            \"statusCode\": 400,\n            \"body\": json.dumps({\"message\": \"User already exists\"}),\n        }\n\n    # Encrypt the password using KMS.\n    encrypted_password = encrypt_with_kms(body[\"password\"], KMS_KEY_ID)\n\n    # Insert the new user into the DynamoDB table.\n    auth_table.put_item(Item={\"PK\": body[\"email\"], \"password\": encrypted_password})\n\n    # Return a successful response with the newly created user ID.\n    return {\"statusCode\": 201}\n</code></pre> <p>This Lambda function basically handles user signup by encrypting passwords with KMS and storing them in DynamoDB, ensuring secure user registration.</p> <p>With our implementation ready, let's configure it to utilize AWS resources for seamless functionality.</p> functions/auth/signup/config.py<pre><code>from infra.services import Services\n\n\nclass SignUpConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"SignUp\",\n            path=\"./functions/auth\",\n            description=\"Securely handle user registration with unique credentials.\",\n            directory=\"signup\",\n            environment={\n                \"AUTH_TABLE_NAME\": services.dynamodb.auth_table.table_name,\n                \"KMS_KEY_ID\": services.kms.auth_key.key_id,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\"POST\", \"/signup\", function, public=True)\n\n        services.dynamodb.auth_table.grant_read_write_data(function)\n\n        services.kms.auth_key.grant_encrypt(function)\n</code></pre>"},{"location":"examples/jwt-authentication/#implementing-the-signin-functionality","title":"Implementing the SignIn Functionality","text":"<p>Now that the signup functionality is in place, let's proceed with the implementation of the signin function. This function will handle user input of email and password, verify them against existing credentials in the database, and decrypt the encrypted password to authenticate the user.</p> <pre><code>forge function signin --method \"POST\" --description \"Authenticate user login by verifying email and password against stored credentials\" --public --belongs-to auth --no-tests --endpoint signin\n</code></pre> <p>Here's our updated folder structure:</p> <pre><code>functions\n\u2514\u2500\u2500 auth\n    \u251c\u2500\u2500 signin\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2514\u2500\u2500 main.py\n    \u2514\u2500\u2500 signup\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>And now, it's implementation.</p> functions/auth/signup/main.py<pre><code>import json\nimport os\nfrom dataclasses import dataclass\n\nimport boto3\nimport jwt\nimport sm_utils\n\n\n@dataclass\nclass Input:\n    email: str\n    password: str\n\n\n@dataclass\nclass Output:\n    token: str\n\n\ndef decrypt_with_kms(ciphertext_blob: bytes, kms_key_id: str) -&gt; str:\n    kms_client = boto3.client(\"kms\")\n\n    # Then you can pass the decoded string to the decrypt method\n    response = kms_client.decrypt(CiphertextBlob=bytes(ciphertext_blob), KeyId=kms_key_id)\n    return response[\"Plaintext\"].decode()\n\n\ndef lambda_handler(event, context):\n    # Retrieve the DynamoDB table name and KMS key ID from environment variables.\n    AUTH_TABLE_NAME = os.environ.get(\"AUTH_TABLE_NAME\")\n    KMS_KEY_ID = os.environ.get(\"KMS_KEY_ID\")\n    JWT_SECRET_NAME = os.environ.get(\"JWT_SECRET_NAME\")\n\n    JWT_SECRET = sm_utils.get_secret(JWT_SECRET_NAME)\n\n    # Parse the request body to get user credentials.\n    body = json.loads(event[\"body\"])\n    email = body[\"email\"]\n    password = body[\"password\"]\n\n    # Initialize a DynamoDB resource.\n    dynamodb = boto3.resource(\"dynamodb\")\n    auth_table = dynamodb.Table(AUTH_TABLE_NAME)\n\n    # Retrieve user data from DynamoDB.\n    response = auth_table.get_item(Key={\"PK\": email})\n    user = response.get(\"Item\")\n\n    # Check if user exists.\n    if not user:\n        return {\"statusCode\": 401, \"body\": json.dumps({\"error\": \"User not found\"})}\n\n    # Check if user exists and password matches.\n    encrypted_password = user.get(\"password\")\n    decrypted_password = decrypt_with_kms(encrypted_password, KMS_KEY_ID)\n\n    # Compare the decrypted password with the provided one.\n    if password == decrypted_password:\n        # Generate JWT token\n        status_code = 200\n        token = jwt.encode({\"email\": email}, JWT_SECRET, algorithm=\"HS256\")\n        body = json.dumps({\"token\": token})\n\n    else:\n        status_code = 401\n        body = json.dumps({\"error\": \"Invalid credentials\"})\n\n    return {\"statusCode\": status_code, \"body\": body}\n</code></pre> <p>Note that upon matching the input password with the encrypted password, the email is encoded within the JWT token and returned to the client, specifically on line 62. This step is crucial for facilitating retrieval of this information at a later stage.</p> <p>Now, let's move on to configure the signin function.</p> functions/auth/signup/config.py<pre><code>from infra.services import Services\n\n\nclass SigninConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Signin\",\n            path=\"./functions/auth\",\n            description=\"Authenticate user login by verifying email and password against stored credentials\",\n            directory=\"signin\",\n            layers=[services.layers.sm_utils_layer, services.layers.pyjwt_layer],\n            environment={\n                \"AUTH_TABLE_NAME\": services.dynamodb.auth_table.table_name,\n                \"KMS_KEY_ID\": services.kms.auth_key.key_id,\n                \"JWT_SECRET_NAME\": services.secrets_manager.jwt_secret.secret_name,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\"POST\", \"/signin\", function, public=True)\n\n        services.dynamodb.auth_table.grant_read_data(function)\n\n        services.kms.auth_key.grant_decrypt(function)\n\n        services.secrets_manager.jwt_secret.grant_read(function)\n</code></pre>"},{"location":"examples/jwt-authentication/#creating-the-jwt-authorizer","title":"Creating the JWT Authorizer","text":"<p>Now that we have the signin function, it returns a token to the client, typically a frontend application, which must include this token in the headers of subsequent requests protected by the JWT authorizer. The authorizer's role is to decode if the token was generated with the same hash as its creation, and if so, decode the token and pass the email to the protected functions.</p> <p>With that being said, let's proceed with its implementation.</p> <pre><code>forge authorizer jwt --description \"A jwt authorizer for private lambda functions\" --no-tests\n</code></pre> <p>This command creates a new <code>jwt</code> authorizer under the <code>authorizers</code> folder.</p> <pre><code>authorizers\n  \u2514\u2500\u2500 jwt\n      \u251c\u2500\u2500 __init__.py\n      \u251c\u2500\u2500 config.py\n      \u2514\u2500\u2500 main.py\n</code></pre> <p>Now, let's proceed with the implementation.</p> authorizers/jwt/main.py<pre><code>import os\nimport jwt\nimport sm_utils\n\ndef lambda_handler(event, context):\n\n    # Extract the JWT token from the event\n    token = event[\"headers\"].get(\"authorization\")\n\n    # Retrieve the JWT secret from Secrets Manager\n    JWT_SECRET_NAME = os.environ.get(\"JWT_SECRET_NAME\")\n    JWT_SECRET = sm_utils.get_secret(JWT_SECRET_NAME)\n\n    try:\n        # Decode the JWT token\n        decoded_token = jwt.decode(token, JWT_SECRET, algorithms=[\"HS256\"])\n        effect = \"allow\"\n        email = decoded_token.get(\"email\")\n    except:\n        effect = \"deny\"\n        email = None\n\n    # Set the decoded email as context\n    context = {\"email\": email}\n\n    # Allow access with the user's email\n    return {\n        \"context\": context,\n        \"policyDocument\": {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Action\": \"execute-api:Invoke\",\n                    \"Effect\": effect,\n                    \"Resource\": event[\"methodArn\"],\n                }\n            ],\n        },\n    }\n</code></pre> <p>This function attempts to decode the token received in the headers under the key <code>authorization</code> using the same JWT secret stored in Secrets Manager that was used during its generation. If successful, it retrieves the hashed email from the signin function and passes it as context.</p> <p>Now, let's set up our new JWT authorizer.</p> authorizers/jwt/config.py<pre><code>from infra.services import Services\n\n\nclass JwtAuthorizerConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"JwtAuthorizer\",\n            path=\"./authorizers/jwt\",\n            description=\"A jwt authorizer for private lambda functions\",\n            layers=[services.layers.sm_utils_layer, services.layers.pyjwt_layer],\n            environment={\n                \"JWT_SECRET_NAME\": services.secrets_manager.jwt_secret.secret_name\n            },\n        )\n\n        services.api_gateway.create_authorizer(function, name=\"jwt\", default=False)\n\n        services.secrets_manager.jwt_secret.grant_read(function)\n</code></pre>"},{"location":"examples/jwt-authentication/#creating-a-private-function","title":"Creating a Private Function","text":"<p>Now it's time to create a simple private function that can only be acessible through requests that passes the validations made through the authorizer.</p> <pre><code>forge function hello --method \"GET\" --description \"A private function\" --no-tests\n</code></pre> <p>This command creates a standalone function in the root of the <code>functions</code> folder.</p> <pre><code>functions\n\u2514\u2500\u2500 hello\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u2514\u2500\u2500 main.py\n</code></pre> <p>Now, let's implement a very straightforward function that should simply retrieve the email decoded by the authorizer and return it to the user.</p> functions/hello/main.py<pre><code>import json\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Input:\n    pass\n\n\n@dataclass\nclass Output:\n    message: str\n\n\ndef lambda_handler(event, context):\n\n    email = event[\"requestContext\"][\"authorizer\"][\"email\"]\n\n    return {\"statusCode\": 200, \"body\": json.dumps({\"message\": f\"Hello, {email}!\"})}\n</code></pre> <p>Finally, it's configuration.</p> functions/hello/config.py<pre><code>from infra.services import Services\n\n\nclass HelloConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Hello\",\n            path=\"./functions/hello\",\n            description=\"A private function\",\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/hello\", function, authorizer=\"jwt\")\n</code></pre> <p>Note that because we didn't specify the JWT authorizer as default, and this function isn't marked as public, we need to explicitly pass the authorizer's name to the <code>create_endpoint</code> method.</p>"},{"location":"examples/jwt-authentication/#deploying-the-functions","title":"Deploying the Functions","text":"<p>Next, we'll commit our code and push it to GitHub, following these steps:</p> <pre><code># Send your changes to stage\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"JWT Authentication System\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>This sequence ensures our code passes through development, staging, and finally, production environments, activating our three distinct deployment pipelines.</p> <p></p> <p>After the pipelines complete, the Authentication system should be available across development, staging, and production stages.</p>"},{"location":"examples/jwt-authentication/#testing-the-functions","title":"Testing the Functions","text":"<p>Let's start by testing the signup function with the credentials below:</p> <ul> <li>Email: <code>tutorial@lambda-forge.com</code></li> <li>Password: <code>12345678</code></li> </ul> <pre><code>curl --request POST \\\n  --url https://api.lambda-forge.com/signup \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"email\": \"tutorial@lambda-forge.com\",\n    \"password\": \"12345678\"\n}'\n</code></pre> <p>The endpoint returns a status code <code>201</code>.</p> <p>However, if we navigate to the <code>Prod-Auth</code> Table on the Dynamo DB console, we'll notice that the password stored isn't simply <code>12345678</code>, but rather a significantly lengthy hash string:</p> <pre><code>AQICAHinYrMBzzQKgEowcHc4llDo3C5gg+cRawehAsWTMZ24iwEvX3NrQs9oYi0hD2YnB28hAAAAZjBkBgkqhkiG9w0BBwagVzBVAgEAMFAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMEeMCuyCVk4C+Nr4OAgEQgCOEKlx01+tGfqKTNXSktApuxUI31EnwzLt7GdW0wdXrT+Yu+A==\n</code></pre> <p>This showcases the robustness of the security measures in place to safeguard passwords.</p> <p>Now, let's utilize the same credentials to log in:</p> <pre><code>curl --request POST \\\n  --url https://api.lambda-forge.com/signin \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"email\": \"tutorial@lambdaforge.com\",\n    \"password\": \"12345678\"\n}'\n</code></pre> <p>The signin endpoint returns a token:</p> <pre><code>{\n  \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InR1dG9yaWFsQGxhbWJkYWZvcmdlLmNvbSJ9.ppQLiYZ-6AtHdwaCb-H-vJnjTCle9ppULqq5-TqVPjk\"\n}\n</code></pre> <p>Next, let's attempt a GET request to the hello function without headers:</p> <pre><code>curl --request GET \\\n  --url https://api.lambda-forge.com/hello\n</code></pre> <p>This returns the message:</p> <pre><code>{\n  \"Message\": \"User is not authorized to access this resource with an explicit deny\"\n}\n</code></pre> <p>However, if we pass the token generated by the signin function:</p> <pre><code>curl --request GET \\\n  --url https://api.lambda-forge.com/hello \\\n  --header 'authorization: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InR1dG9yaWFsQGxhbWJkYWZvcmdlLmNvbSJ9.ppQLiYZ-6AtHdwaCb-H-vJnjTCle9ppULqq5-TqVPjk'\n</code></pre> <p>We receive the desired output:</p> <pre><code>{\n  \"message\": \"Hello, tutorial@lambda-forge.com!\"\n}\n</code></pre> <p>\ud83c\udf89 Congratulations! You've successfully implemented a JWT authentication system, securing your endpoints.\ud83d\udd12</p>"},{"location":"examples/real-time-chat/","title":"Building a Real-Time Chat Application with WebSockets in a Serverless Architecture","text":"<p>In this section, we will develop a real-time chat application using WebSockets, facilitating instant communication. This solution is versatile and can be adapted to different scenarios that require fast, real-time interactions, such as the live chat application shown below.</p> <p>The architecture for the Lambda functions we are going to implement will be as follows:</p> <p> </p>"},{"location":"examples/real-time-chat/#incorporating-the-websockets-class-into-the-services-class","title":"Incorporating the Websockets Class into the Services Class","text":"<p>Traditionally, integrating WebSockets involves a significant amount of setup, including configuring the API and establishing various routes to initiate a connection. Fortunately, Forge simplifies this process considerably by offering a template where the connections are pre-configured and interconnected.</p> <pre><code>forge service websockets\n</code></pre> <p>This command creates a new file within the <code>infra/services</code> directory specifically for managing Websocket connections.</p> <pre><code>infra\n\u2514\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u251c\u2500\u2500 dynamodb.py\n    \u251c\u2500\u2500 kms.py\n    \u251c\u2500\u2500 layers.py\n    \u251c\u2500\u2500 s3.py\n    \u251c\u2500\u2500 secrets_manager.py\n    \u2514\u2500\u2500 websockets.py\n</code></pre> <p>This class is crafted to simplify the traditionally complex process of creating, deploying, and integrating WebSocket APIs. It not only streamlines the setup but also manages the necessary permissions, enabling functions to seamlessly send messages through a WebSocket channel.</p> <p>To ease the configuration and management of WebSockets, we utilize the open-source library <code>b_aws_websocket_api</code>, enhancing our integration capabilities and efficiency.</p>"},{"location":"examples/real-time-chat/#setup-the-websocket-urls-for-a-multi-stage-environment","title":"Setup the Websocket URLS for a Multi-Stage Environment","text":"<p>Before we proceed, it's important to note a key consideration. Similar to the URL Shortener project, our Lambda operates in a multi-staging environment. This setup requires unique WebSocket URLs for each stage to ensure proper functionality. To manage this, we must specify these distinct URLs in the <code>cdk.json</code> file, assigning a unique URL to each stage accordingly.</p> <p>Note</p> <p>If you have configured a custom domain name, you can preemptively specify your URLs. If not, you will need to deploy your code to AWS to obtain the WebSocket URLs. Subsequently, these URLs must be added to the cdk.json file.</p> cdk.json<pre><code>   \"dev\": {\n      \"base_url\": \"https://api.lambda-forge.com/dev\",\n      \"post_to_connection_url\": \"$DEV-POST-TO-CONNECTION-URL\",\n      \"arns\": {\n        \"urls_table\": \"$DEV-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$DEV-IMAGES-BUCKET-ARN\",\n        \"auth_table\": \"$DEV-AUTH-TABLE-ARN\"\n      }\n    },\n    \"staging\": {\n      \"base_url\": \"https://api.lambda-forge.com/staging\",\n      \"post_to_connection_url\": \"$STAGING-POST-TO-CONNECTION-URL\",\n      \"arns\": {\n        \"urls_table\": \"$STAGING-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$STAGING-IMAGES-BUCKET-ARN\",\n        \"auth_table\": \"$STAGING-AUTH-TABLE-ARN\"\n      }\n    },\n    \"prod\": {\n      \"base_url\": \"https://api.lambda-forge.com\",\n      \"post_to_connection_url\": \"$PROD-POST-TO-CONNECTION-URL\",\n      \"arns\": {\n        \"urls_table\": \"$PROD-URLS-TABLE-ARN\",\n        \"images_bucket\": \"$PROD-IMAGES-BUCKET-ARN\",\n        \"auth_table\": \"$PROD-AUTH-TABLE-ARN\"\n      }\n    }\n</code></pre>"},{"location":"examples/real-time-chat/#establishing-a-connection-with-the-websocket","title":"Establishing a Connection with the Websocket","text":"<p>To identify the connection IDs for each WebSocket channel, it's essential to inform the client of our application about their respective IDs. This step ensures we can accurately locate the channels as they begin to exchange messages.</p> <p>Within the AWS environment, we face a specific constraint that we must adhere to: We cannot directly send messages to a WebSocket channel from the Lambda function that handles the connection.</p> <p>Therefore, we must implement a sequential two-part process. The initial Lambda function, assigned to handle the connection setup, will trigger another Lambda function. This second function is specifically designed with the capability to send messages directly through the WebSocket channel.</p>"},{"location":"examples/real-time-chat/#creating-the-function-to-send-the-connection-ids","title":"Creating the Function to Send the Connection IDs","text":"<p>Let's first create the function that will actually send the connection id.</p> <pre><code>forge function send_connection_id --description \"Sends the connection id to the client when a connection is made\" --belongs-to \"chat\" --no-tests\n</code></pre> <p>This command creates a new <code>send_connection_id</code> function within the <code>chat</code> directory.</p> <pre><code>functions\n\u2514\u2500\u2500 chat\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 send_connection_id\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>This function is going to be very simple, it should simply receive an event with the connection id and send a message to the desired websocket channel.</p> functions/chat/send_connection_id/main.py<pre><code>import json\nimport os\nimport boto3\n\n\ndef lambda_handler(event, context):\n\n    # Retrieve the connection ID from the event\n    connection_id = event[\"connection_id\"]\n\n    # Create a client for the API Gateway Management API\n    api_gateway_management_client = boto3.client(\n        \"apigatewaymanagementapi\", endpoint_url=os.environ.get(\"POST_TO_CONNECTION_URL\")\n    )\n\n    # Send the payload to the WebSocket\n    api_gateway_management_client.post_to_connection(\n        ConnectionId=connection_id, Data=json.dumps({\"connection_id\": connection_id}).encode(\"utf-8\")\n    )\n\n    return {\"statusCode\": 200}\n</code></pre> <p>Warning</p> <p>When working with WebSockets, it's crucial to return the status code in your responses. Failing to do so may lead to errors in your application.</p> <p>We need to implement a minor adjustment to the Lambda stack, ensuring it passes the context to the <code>SendConnectionIdConfig</code> class. This modification allows the function to dynamically determine the appropriate environment for message delivery.</p> infra/stacks/lambda_stack.py<pre><code>        # Chat\n        SendConnectionIdConfig(self.services, context)\n</code></pre> <p>Now, Let's configure it.</p> functions/chat/send_connection_id/config.py<pre><code>from infra.services import Services\nfrom aws_cdk import aws_iam as iam\n\n\nclass SendConnectionIdConfig:\n    def __init__(self, services: Services, context) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"SendConnectionId\",\n            path=\"./functions/chat\",\n            description=\"Sends the connection id to the client when a connection is made\",\n            directory=\"send_connection_id\",\n            environment={\n              \"POST_TO_CONNECTION_URL\": context.resources[\"post_to_connection_url\"]\n            },\n        )\n\n        function.add_to_role_policy(\n            iam.PolicyStatement(\n                actions=[\"execute-api:ManageConnections\"],\n                resources=[\"arn:aws:execute-api:*:*:*\"],\n            )\n        )\n</code></pre>"},{"location":"examples/real-time-chat/#creating-the-function-to-handle-the-websocket-connections","title":"Creating the Function to Handle the Websocket Connections","text":"<p>With the function for sending the connections id now established, let's proceed to set up the connection handler.</p> <pre><code>forge function connect --description \"Handle the websocket connection\" --websocket --belongs-to \"chat\" --no-tests\n</code></pre> <p>As expected, a new function has been created on the <code>chat</code> directory.</p> <pre><code>functions\n\u2514\u2500\u2500 chat\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 connect\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2514\u2500\u2500 main.py\n    \u2514\u2500\u2500 send_connection_id\n       \u251c\u2500\u2500 __init__.py\n       \u251c\u2500\u2500 config.py\n       \u2514\u2500\u2500 main.py\n</code></pre> <p>Let's move forward with its straightforward implementation, which will essentially involve invoking another function and passing along the connection ID.</p> functions/chat/connect/main.py<pre><code>import json\nimport os\nimport boto3\n\n\n\ndef lambda_handler(event, context):\n\n    # Retrieve the connection ID from the request context\n    connection_id = event[\"requestContext\"][\"connectionId\"]\n\n    # Create a client for the AWS Lambda service\n    lambda_client = boto3.client(\"lambda\")\n\n    # Retrieve the ARN of the target Lambda function from the environment variables\n    TARGET_FUNCTION_ARN = os.environ.get(\"TARGET_FUNCTION_ARN\")\n\n    # Define the payload to pass to the target Lambda function\n    payload = {\"connection_id\": connection_id}\n\n    # Invoke the target Lambda function asynchronously\n    lambda_client.invoke(FunctionName=TARGET_FUNCTION_ARN, InvocationType=\"Event\", Payload=json.dumps(payload))\n\n    return {\"statusCode\": 200}\n</code></pre> <p>This function has a crucial dependency on another, requiring precise configuration to grant it the necessary permissions for invoking the target function and to correctly obtain its ARN.</p> <p>To manage such scenarios, the <code>AWSLambda</code> class maintains references to all functions it creates. This approach ensures these functions remain accessible for future interactions.</p> <p>With this context in mind, let's proceed to set up the connection handler.</p> functions/chat/connect/config.py<pre><code>from infra.services import Services\n\n\nclass ConnectConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        send_connection_id_function = services.aws_lambda.functions[\"SendConnectionId\"]\n\n        connect_function = services.aws_lambda.create_function(\n            name=\"Connect\",\n            path=\"./functions/chat\",\n            description=\"Handle the websocket connection\",\n            directory=\"connect\",\n            environment={\n                \"TARGET_FUNCTION_ARN\": send_connection_id_function.function_arn,\n            },\n        )\n\n        services.websockets.create_route(\"$connect\", connect_function)\n\n        send_connection_id_function.grant_invoke(connect_function)\n</code></pre> <p>To ensure the <code>SendConnectionId</code> function is available in the configuration for the <code>Connect</code> function, it's crucial to define the target function before creating the trigger function.</p> <p>The order in which functions are generated and become operational is directly tied to their definition sequence within the <code>LambdaStack</code>. Hence, it is imperative to ensure the sequence is correctly arranged, with the target function being established before its corresponding trigger function.</p> infra/stacks/lambda_stack.py<pre><code>        # Chat\n        SendConnectionIdConfig(self.services, context)\n        ConnectConfig(self.services)\n</code></pre>"},{"location":"examples/real-time-chat/#sending-messages","title":"Sending Messages","text":"<p>Once a connection is established, we can be sending messages in real time to each channel. So let's create the function that will send the messages.</p> <pre><code>forge function send_message --description \"Send messages to sender and recipient\" --websocket --belongs-to \"chat\" --no-tests\n</code></pre> <p>Below is the updated layout of our folder structure.</p> <pre><code>functions\n\u2514\u2500\u2500 chat\n    \u251c\u2500\u2500 connect\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2514\u2500\u2500 main.py\n    \u251c\u2500\u2500 send_connection_id\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2514\u2500\u2500 main.py\n    \u2514\u2500\u2500 send_message\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>Like the previous functions, this one will be very simple. It will accept a connection ID for posting messages, using the channel ID as the sender ID. Additionally, it will forward the same messages to both channels, ensuring that clients can maintain a message history across both channels.</p> functions/chat/send_message/main.py<pre><code>import json\nimport os\nimport boto3\n\n\ndef lambda_handler(event, context):\n\n    # Retrieve the URL for posting messages to connected clients from the environment variables\n    POST_TO_CONNECTION_URL = os.environ.get(\"POST_TO_CONNECTION_URL\")\n\n    # Create a client for the API Gateway Management API, specifying the endpoint URL\n    apigtw_management = boto3.client(\n        \"apigatewaymanagementapi\",\n        endpoint_url=POST_TO_CONNECTION_URL,\n    )\n\n    # Retrieve the connection ID of the sender from the request context\n    sender_id = event[\"requestContext\"][\"connectionId\"]\n\n    # Parse the incoming message and the recipient ID from the Lambda event body\n    message = json.loads(event[\"body\"])[\"message\"]\n    recipient_id = json.loads(event[\"body\"])[\"recipient_id\"]\n\n    # Iterate over both the sender and recipient connection IDs\n    for connection_id in [sender_id, recipient_id]:\n        apigtw_management.post_to_connection(\n            ConnectionId=connection_id,\n            Data=json.dumps({\"message\": message, \"sender_id\": sender_id, \"recipient_id\": recipient_id}),\n        )\n\n    return {\"statusCode\": 200}\n</code></pre> <p>Similar to the <code>SendConnectionId</code> function, it's necessary to pass the context from the <code>LambdaStack</code> to the <code>SendMessageConfig</code> class. This step enables our function to discern the appropriate stage for message dispatch.</p> infra/stacks/lambda_stack.py<pre><code>        # Chat\n        SendMessageConfig(self.services, context)\n        SendConnectionIdConfig(self.services, context)\n        ConnectConfig(self.services)\n</code></pre> <p>Having the context available, we can now configure the send message function.</p> functions/chat/send_message/config.py<pre><code>from infra.services import Services\nfrom aws_cdk import aws_iam as iam\n\n\nclass SendMessageConfig:\n    def __init__(self, services: Services, context) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"SendMessage\",\n            path=\"./functions/chat\",\n            description=\"Send messages to sender and recipient\",\n            directory=\"send_message\",\n            environment={\n                \"POST_TO_CONNECTION_URL\": context.resources[\"post_to_connection_url\"],\n            },\n        )\n\n        services.websockets.create_route(\"sendMessage\", function)\n\n        function.add_to_role_policy(\n            iam.PolicyStatement(\n                actions=[\"execute-api:ManageConnections\"],\n                resources=[f\"arn:aws:execute-api:*:*:*\"],\n            )\n        )\n</code></pre>"},{"location":"examples/real-time-chat/#deploying-the-functions","title":"Deploying the Functions","text":"<p>Next, we'll commit our code and push it to GitHub, following these steps:</p> <pre><code># Send your changes to stage\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"Serverless Real-Time Chat Application\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>This sequence guarantees that our code progresses through the development, staging, and ultimately, the production environments, triggering our three separate deployment pipelines in the process.</p> <p></p> <p>Upon completion of these pipelines, the Real Time Chat will be operational across the development, staging, and production stages.</p>"},{"location":"examples/real-time-chat/#testing-the-functions","title":"Testing the Functions","text":"<p>To thoroughly test our real-time application, we'll connect to a WebSocket client. While there are numerous clients available for testing WebSockets, we'll utilize PieSocket's WebSocket tester, available at PieSocket WebSocket Tester.</p> <p>The URL we will use to test our application corresponds to the development stage, as detailed below:</p> <p>wss://2clnav84i7.execute-api.us-east-2.amazonaws.com/dev/</p>"},{"location":"examples/real-time-chat/#step-1-establish-a-websocket-connection","title":"Step 1: Establish a WebSocket Connection","text":"<p>Navigate to the PieSocket tester and enter the URL of your WebSocket connection. This URL is typically generated after deploying your WebSocket service.</p> <p></p> <p>Upon successfully connecting, the WebSocket service will respond with the connection ID.</p> <p></p>"},{"location":"examples/real-time-chat/#step-2-create-a-second-connection","title":"Step 2: Create a Second Connection","text":"<p>Open a new tab or window and repeat the connection steps to establish a second WebSocket client.</p> <p></p> <p>You'll notice each connection has a unique ID. These IDs are crucial for directing messages to the correct recipient.</p>"},{"location":"examples/real-time-chat/#step-3-test-messaging-between-connections","title":"Step 3: Test Messaging Between Connections","text":"<p>Choose one tab as the sender and use the other's connection ID as the recipient. You can send a message by inputting a JSON payload like the one below, substituting <code>$CONNECTION_ID</code> with the actual connection ID of the recipient:</p> <pre><code>{\n  \"action\": \"sendMessage\",\n  \"message\": \"Hello\",\n  \"recipient_id\": \"$CONNECTION_ID\"\n}\n</code></pre> <p>If everything is configured correctly, you should observe the message appearing in real-time on both channels!</p> <p> </p> <p>\ud83c\udf89 Congratulations! You've now successfully developed a Serverless Real-Time Chat application using WebSockets and Lambda Forge. \ud83d\udcac \ud83c\udf10</p>"},{"location":"examples/url-shortener/","title":"Creating a URL Shortener Service Using DynamoDB","text":"<p>In this section, we will explore the development of a URL shortener. This utility enables users to input a lengthy URL, which the system then compresses into a more concise version.</p> <p>The setup for the Lambda functions we aim to develop will follow this structure:</p> <p> </p>"},{"location":"examples/url-shortener/#setting-up-the-dynamodb-tables","title":"Setting Up the DynamoDB Tables","text":"<p>To begin, create three separate DynamoDB tables in the AWS Console to store URLs for different environments: <code>Dev-URLs</code>, <code>Staging-URLs</code>, and <code>Prod-URLs</code>.</p> <p>Next, open your project\u2019s <code>cdk.json</code> file and incorporate the tables ARNs into the respective environment configurations. This setup ensures that each environment interacts only with its designated table, maintaining clear separation and organization.</p> cdk.json<pre><code>    \"dev\": {\n      \"arns\": {\n        \"numbers_table\": \"$DEV-NUMBERS-TABLE-ARN\",\n        \"urls_table\": \"$DEV-URLS-TABLE-ARN\",\n      }\n    },\n    \"staging\": {\n      \"arns\": {\n        \"numbers_table\": \"$STAGING-NUMBERS-TABLE-ARN\",\n        \"urls_table\": \"$STAGING-URLS-TABLE-ARN\",\n      }\n    },\n    \"prod\": {\n      \"arns\": {\n        \"numbers_table\": \"$PROD-NUMBERS-TABLE-ARN\",\n        \"urls_table\": \"$PROD-URLS-TABLE-ARN\",\n      }\n    }\n</code></pre> <p>Next, we need to create a new variable class within the DynamoDB class to reference our URLs tables.</p> infra/services/dynamodb.py<pre><code>class DynamoDB:\n    def __init__(self, scope, context: dict) -&gt; None:\n\n        self.numbers_table = dynamodb.Table.from_table_arn(\n            scope,\n            \"NumbersTable\",\n            context.resources[\"arns\"][\"numbers_table\"],\n        )\n\n        self.urls_table = dynamodb.Table.from_table_arn(\n            scope,\n            \"URLsTable\",\n            context.resources[\"arns\"][\"urls_table\"],\n        )\n</code></pre>"},{"location":"examples/url-shortener/#implementing-the-shortener-function","title":"Implementing the Shortener Function","text":"<p>To initiate, let's develop the shortener function, which serves as the primary interface for user interaction. This function is tasked with accepting a lengthy URL from the user and providing them with its shortened counterpart in response:</p> <pre><code>forge function shortener --method \"POST\" --description \"Creates a new short URL entry in DynamoDB mapped to the original one\" --belongs-to urls --public --no-tests\n</code></pre> <p>Executing the command will result in the establishment of the following directory structure:</p> <pre><code>functions\n\u2514\u2500\u2500 urls\n    \u2514\u2500\u2500  shortener\n       \u251c\u2500\u2500 __init__.py\n       \u251c\u2500\u2500 config.py\n       \u2514\u2500\u2500  main.py\n</code></pre> <p>Now, let's implement it's functionality:</p> functions/urls/shortener/main.py<pre><code>from dataclasses import dataclass\nimport hashlib\nimport json\nimport os\nimport boto3\n\n@dataclass\nclass Input:\n    url: str\n\n\n@dataclass\nclass Output:\n    short_url: str\n\n\ndef lambda_handler(event, context):\n    # Retrieve DynamoDB table name and the Base URL from environment variables.\n    URLS_TABLE_NAME = os.environ.get(\"URLS_TABLE_NAME\")\n    BASE_URL = os.environ.get(\"BASE_URL\")\n\n    # Initialize DynamoDB resource.\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the specified DynamoDB table.\n    urls_table = dynamodb.Table(URLS_TABLE_NAME)\n\n    # Parse the URL from the incoming event's body.\n    body = json.loads(event[\"body\"])\n    original_url = body[\"url\"]\n\n    # Generate a URL hash.\n    hash_object = hashlib.sha256(original_url.encode())\n    url_id = hash_object.hexdigest()[:6]\n\n    # Store the mapping in DynamoDB.\n    urls_table.put_item(Item={\"PK\": url_id, \"original_url\": original_url})\n\n    # Construct the shortened URL.\n    short_url = f\"{BASE_URL}/{url_id}\"\n\n    # Return success response.\n    return {\"statusCode\": 200, \"body\": json.dumps({\"short_url\": short_url})}\n</code></pre> <p>This code is the core of our URL shortening service. It transforms long URLs into shorter, hash-based versions, and storing this information in DynamoDB for future retrieval.</p> <p>Since we are operating in a multi-stage environment, this function is dynamically retrieving the BASE URL from environment variables, as shown on line 20. This approach ensures stage-specific responses, enabling seamless URL customization.</p> <p>To make this possible, we must incorporate the base URL into the <code>cdk.json</code> file and implement minor modifications. These adjustments will enable the base URL to be accessible within the <code>config.py</code> class, thereby allowing the function to access the appropriate base URL depending on the environment it's operating in.</p> cdk.json<pre><code>    \"dev\": {\n      \"base_url\": \"https://api.lambda-forge.com/dev\",\n      \"arns\": {\n        \"numbers_table\": \"$DEV-NUMBERS-TABLE-ARN\",\n        \"urls_table\": \"$DEV-URLS-TABLE-ARN\"\n      }\n    },\n    \"staging\": {\n      \"base_url\": \"https://api.lambda-forge.com/staging\",\n      \"arns\": {\n        \"numbers_table\": \"$STAGING-NUMBERS-TABLE-ARN\",\n        \"urls_table\": \"$STAGING-URLS-TABLE-ARN\"\n      }\n    },\n    \"prod\": {\n      \"base_url\": \"https://api.lambda-forge.com\",\n      \"arns\": {\n        \"numbers_table\": \"$PROD-NUMBERS-TABLE-ARN\",\n        \"urls_table\": \"$PROD-URLS-TABLE-ARN\"\n      }\n    }\n</code></pre> <p>Note</p> <p> Follow the article Locating The Api Gateway Base URL on CloudFormation to locate your own base URL in each environment. </p> <p>Initially, the <code>LambdaStack</code> class sends only the <code>self.services</code> as argument to the <code>ShortenerConfig</code> class. We must update it to also send the <code>context</code> parameter. This change allows the config class to access base URLs and dynamically set the correct environment variables during the function definition, enhancing its adaptability.</p> infra/stacks/lambda_stack.py<pre><code>class LambdaStack(Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n\n        super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs)\n\n        self.services = Services(self, context)\n\n        # GuessTheNumber\n        MakeGuessConfig(self.services)\n        CreateGameConfig(self.services)\n\n        # Urls\n        ShortenerConfig(self.services, context)\n</code></pre> <p>To conclude, we will now proceed with configuring our Lambda function.</p> functions/urls/config.py<pre><code>from infra.services import Services\n\nclass ShortenerConfig:\n    def __init__(self, services: Services, context) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Shortener\",\n            path=\"./functions/urls\",\n            description=\"Creates a new short URL entry in DynamoDB mapping to the original url\",\n            directory=\"shortener\",\n            environment={\n                \"URLS_TABLE_NAME\": services.dynamodb.urls_table.table_name,\n                \"BASE_URL\": context.resources[\"base_url\"],\n            },\n        )\n\n        services.api_gateway.create_endpoint(\"POST\", \"/urls\", function, public=True)\n\n        services.dynamodb.grant_write(\"numbers_table\", function)\n</code></pre> <p>In this configuration, we specify resources according to the deployment stages of the Lambda function, setting up the DynamoDB table and API Gateway base URL accordingly. It also includes permission settings, enabling the Lambda function to write to our DynamoDB table.</p>"},{"location":"examples/url-shortener/#implementing-the-redirect-function","title":"Implementing the Redirect Function","text":"<p>Having established the necessary components for URL shortening, we now proceed to create a new function tasked with redirecting users from the shortened URL to its original counterpart.</p> <p>Begin by creating a new function:</p> <pre><code>forge function redirect --method \"GET\" --description \"Redirects from the short url to the original url\" --belongs-to urls --public --no-tests\n</code></pre> <p>The revised directory structure will appear as follows:</p> <pre><code>functions\n\u2514\u2500\u2500 urls\n    \u251c\u2500\u2500 redirect\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2514\u2500\u2500 main.py\n    \u2514\u2500\u2500 shortener\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py\n        \u2514\u2500\u2500 main.py\n</code></pre> <p>Now, let's implement the redirect functionality.</p> functions/urls/redirect/main.py<pre><code>from dataclasses import dataclass\nimport json\nimport boto3\nimport os\n\n\n@dataclass\nclass Path:\n    url_id: str\n\n\n@dataclass\nclass Input:\n    pass\n\n\n@dataclass\nclass Output:\n    pass\n\n\ndef lambda_handler(event, context):\n\n    # Retrieve DynamoDB table name from environment variables.\n    URLS_TABLE_NAME = os.environ.get(\"URLS_TABLE_NAME\")\n\n    # Initialize DynamoDB resource and table reference.\n    dynamodb = boto3.resource(\"dynamodb\")\n    urls_table = dynamodb.Table(URLS_TABLE_NAME)\n\n    # Extract shortened URL identifier from path parameters.\n    short_url = event[\"pathParameters\"][\"url_id\"]\n\n    # Retrieve the original URL using the shortened identifier.\n    response = urls_table.get_item(Key={\"PK\": short_url})\n    original_url = response.get(\"Item\", {}).get(\"original_url\")\n\n    # Return 404 if no URL is found for the identifier.\n    if original_url is None:\n        return {\"statusCode\": 404, \"body\": json.dumps({\"message\": \"URL not found\"})}\n\n    # Ensure URL starts with \"http://\" or \"https://\".\n    if not original_url.startswith(\"http\"):\n        original_url = f\"http://{original_url}\"\n\n    # Redirect to the original URL with a 301 Moved Permanently response.\n    return {\"statusCode\": 301, \"headers\": {\"Location\": original_url}}\n</code></pre> <p>In this Lambda function, we're essentially setting up a redirect service. When a request comes in with a short URL identifier, the function looks up this identifier in the DynamoDB table to find the corresponding original URL. If found, it redirects the user to the original URL.</p> <p>Next, let's move on to its configuration.</p> functions/urls/redirect/config.py<pre><code>from infra.services import Services\n\n\nclass RedirectConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Redirect\",\n            path=\"./functions/urls\",\n            description=\"Redirects from the short url to the original url\",\n            directory=\"redirect\",\n            environment={\n                \"URLS_TABLE_NAME\": services.dynamodb.urls_table.table_name,\n            }\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/{url_id}\", function, public=True)\n\n        services.dynamodb.urls_table.grant_read_data(function)\n</code></pre>"},{"location":"examples/url-shortener/#deploying-the-functions","title":"Deploying the Functions","text":"<p>Next, we'll commit our code and push it to GitHub, following these steps:</p> <pre><code># Send your changes to stage\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"URL Shortener with DynamoDB integration\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>This sequence ensures our code passes through development, staging, and finally, production environments, activating our three distinct deployment pipelines.</p> <p></p> <p>After the pipelines complete, the URL Shortener feature is available across development, staging, and production stages.</p>"},{"location":"examples/url-shortener/#testing-the-deployment","title":"Testing The Deployment","text":"<p>Let's test our URL Shortener by shortening a lengthy URL. For demonstration purposes, we'll use the production environment, but the process remains identical for development and staging, using their respective endpoints.</p> <p>Execute a POST request to shorten the URL:</p> <pre><code>curl --request POST \\\n  --url https://api.lambda-forge.com/urls \\\n  --header 'Content-Type: application/json' \\\n  --header 'accept: application/json' \\\n  --data '{\n    \"url\": \"https://public-lambda-forge-logo.s3.us-east-2.amazonaws.com/wNSN2U7n9NiAKEItWlsrcdJ0RWFyZOmbNvsc6Kht84WsWVxuBz5O.png\"\n}'\n</code></pre> <p>This request generates a short URL:</p> <pre><code>{\n  \"short_url\": \"https://api.lambda-forge.com/bc23d3\"\n}\n</code></pre> <p>Navigating to this URL in your browser will redirect you to the original content, showcasing our URL Shortener in action.</p> <p></p> <p>\ud83c\udf89 Success! Our URL shortener function is now deployed and operational across all environments.</p>"},{"location":"home/aws-services/","title":"Integrating AWS Services to Lambda Functions","text":"<p>Lambda functions frequently interact with various AWS services. With AWS offering over 200 services, connecting these functions to leverage different services, creating triggers, and assigning the proper permissions can be quite challenging. To address this complexity and to create a scalable architecture that can evolve over time, Lambda Forge employs dependency injection. This approach injects each service with its single responsibility into our functions, simplifying management and enhancing scalability.</p>"},{"location":"home/aws-services/#the-services-class","title":"The Services Class","text":"<p>Every AWS resources is defined in separate classes within the <code>infra/services</code> folder. When you initiate a new project using <code>forge project</code>, this directory is automatically created and populated with two service directories: <code>api_gateway</code> and <code>aws_lambda</code>. Each file contains template code to facilitate the creation of a Lambda function and its integration with API Gateway.</p> <pre><code>infra\n\u2514\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u2514\u2500\u2500 aws_lambda.py\n</code></pre> <p>Within the <code>infra/services/__init__.py</code> file, you'll find the <code>Services</code> class, a comprehensive resource manager designed to streamline the interaction with AWS services. This class acts as a dependency injector, enabling the easy and efficient configuration of AWS resources.</p> infra/services/__init__.py<pre><code>from infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\n\nclass Services:\n\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n</code></pre>"},{"location":"home/aws-services/#creating-aws-services","title":"Creating AWS Services","text":"<p>Lambda Forge simplifies working with AWS Lambda functions by providing pre-built classes for common AWS services. To create these classes, use the following command:</p> <p><code>forge service $SERVICE</code></p> <p>Lambda Forge currently has built-in classes for the following AWS services:</p> API Gateway Lambda Layers AWS Lambdas KMS Cognito S3 DynamoDB Secrets Manager Event Bridge SNS Websockets SQS <p>Note</p> <p> You can create your own custom classes using CDK for handling other AWS services by following the architecture set by Lambda Forge, which adheres to the Single Responsibility Principle and Dependency Injection. This approach allows you to seamlessly integrate your custom service class into the existing framework. </p>"},{"location":"home/aws-services/#example-creating-an-sns-class","title":"Example: Creating an SNS Class","text":"<p>To create a class for handling SNS configurations, run <code>forge service sns</code>. This command will update your folder structure as shown below:</p> <pre><code>infra\n\u2514\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u2514\u2500\u2500 sns.py\n</code></pre> <p>Here's an example of the SNS class provided by Lambda Forge:</p> infra/services/sns.py<pre><code>from aws_cdk import aws_lambda_event_sources\nfrom aws_cdk import aws_sns as sns\n\nfrom lambda_forge.trackers import invoke, trigger\n\n\nclass SNS:\n    def __init__(self, scope, context) -&gt; None:\n\n        # self.sns_topic = sns.Topic.from_topic_arn(\n        #     scope,\n        #     id=\"SNSTopic\",\n        #     topic_arn=context.resources[\"arns\"][\"sns_topic_arn\"],\n        # )\n        ...\n\n    @trigger(service=\"sns\", trigger=\"topic\", function=\"function\")\n    def create_trigger(self, topic, function):\n        topic = getattr(self, topic)\n        sns_subscription = aws_lambda_event_sources.SnsEventSource(topic)\n        function.add_event_source(sns_subscription)\n\n\n    @invoke(service=\"sns\", resource=\"topic\", function=\"function\")\n    def grant_publish(self, topic, function):\n        topic = getattr(self, topic)\n        topic.grant_publish(function)\n</code></pre> <p>The SNS class provided by Forge includes methods for triggering a lambda and granting it permission to consume a topic. These methods are designed to keep track of these events using the <code>trigger</code> and <code>invoke</code> decorators.</p>"},{"location":"home/aws-services/#updating-hello-world-function-to-also-be-triggered-by-sns","title":"Updating Hello World Function to also be Triggered by SNS","text":"<p>For example, let's update our already existent <code>Hello World</code> function to be triggered by both the API Gateway and the SNS topic. </p> <p>In order to do so, we need modify the SNS class adding the ARN of the real topic from AWS:</p> infra/services/sns.py<pre><code>class SNS:\n    def __init__(self, scope, context) -&gt; None:\n\n        self.hello_world_topic = sns.Topic.from_topic_arn(\n            scope,\n            id=\"HelloWorldTopic\",\n            topic_arn=context.resources[\"arns\"][\"hello_world_topic\"],\n        )\n</code></pre> <p>Now we need to define the topic arns for each environment in the <code>cdk.json</code> file:</p> cdk.json<pre><code>    \"dev\": {\n      \"arns\": {\n          \"hello_world_topic\": \"$DEV-SNS-TOPIC\"\n      }\n    },\n    \"staging\": {\n        \"arns\": {\n            \"hello_world_topic\": \"$STAGING-SNS-TOPIC\",\n        }\n    },\n    \"prod\": {\n        \"arns\": {\n            \"hello_world_topic\": \"$PROD-SNS-TOPIC\",\n        }\n    }\n</code></pre> <p>Now, change the existing <code>config.py</code> file to also use the new SNS topic:</p> functions/hello_world/config.py<pre><code>from infra.services import Services\n\n\nclass HelloWorldConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"HelloWorld\",\n            path=\"./functions/hello_world\",\n            description=\"A simple hello world\",\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/hello_world\", function, public=True)\n\n        services.sns.create_trigger(\"hello_world_topic\", function)\n</code></pre> <p>Now our hello world function can be triggered by two different sources.</p>"},{"location":"home/aws-services/#tracking-the-triggers-and-invocations","title":"Tracking the Triggers and Invocations","text":"<p>Every time we run the command <code>cdk synth</code>, Lambda Forge tracks the trigger and invocations for each lambda defined in your Lambda Stack and generate a json file called <code>functions.json</code> file at the root of your project.</p> functions.json<pre><code>[\n    {\n        \"name\": \"HelloWorld\",\n        \"path\": \"./functions/hello_world\",\n        \"description\": \"A simple hello world\",\n        \"timeout\": 60,\n        \"triggers\": [\n            {\n                \"service\": \"api_gateway\",\n                \"trigger\": \"/hello_world\",\n                \"method\": \"GET\",\n                \"public\": true\n            },\n            {\n                \"service\": \"sns\",\n                \"trigger\": \"hello_world_topic\"\n            }\n        ],\n        \"invocations\": []\n    }\n]\n</code></pre> <p>The <code>functions.json</code> file is essential to Lambda Forge, serving as a crucial component for many of its features, including documentation generation and custom CodePipeline steps.</p>"},{"location":"home/creating-a-hello-world/","title":"Creating a Public Hello World Function With API Gateway","text":"<p>Creating a Hello World function is a great way to get started with Lambda Forge. This function will serve as a simple demonstration of Lambda Forge's ability to quickly deploy serverless functions accessible via an HTTP endpoint.</p> <p>Here's how you can create your first public Hello World function.</p> <pre><code>forge function hello_world --method \"GET\" --description \"A simple hello world\" --public\n</code></pre> <p>This command prompts Lambda Forge to initiate a new Lambda function located in the <code>hello_world</code> directory. The <code>--method</code> parameter defines the HTTP method accessible for this function.. The <code>--description</code> option provides a concise summary of the function\u2019s intent, and the <code>--public</code> flag ensures the function is openly accessible, allowing it to be invoked by anyone who has the URL.</p>"},{"location":"home/creating-a-hello-world/#function-structure","title":"Function Structure","text":"<p>When you create a new function with Lambda Forge, it not only simplifies the creation process but also sets up a robust and organized file structure for your function. This structure is designed to support best practices in software development, including separation of concerns, configuration management, and testing. Let's break down the structure of the automatically generated hello_world function:</p> <pre><code>functions/\n\u2514\u2500\u2500 hello_world/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 integration.py\n    \u251c\u2500\u2500 main.py\n    \u2514\u2500\u2500 unit.py\n</code></pre> <ul> <li><code>functions/</code> This directory is the root folder for all your Lambda functions. Each function has its own subdirectory within this folder.</li> <li><code>hello_world/</code> The hello_world subdirectory contains all the necessary files for your function to run, be configured, and tested.</li> <li><code>config.py</code> Holds the configuration settings for the function. These might include environment variables, resource identifiers, and other parameters critical for the function's operation.</li> <li><code>integration.py</code> Contains integration tests that simulate the interaction of your function with external services or resources.</li> <li><code>main.py</code> This is where the core logic of your Lambda function resides. The handler function, which AWS Lambda invokes when the function is executed, is defined here.</li> <li><code>unit.py</code> Contains unit tests for your function. Unit tests focus on testing individual parts of the function's code in isolation, ensuring that each component behaves as expected.</li> </ul>"},{"location":"home/creating-a-hello-world/#implementing-the-hello-world-function","title":"Implementing the Hello World Function","text":"<p>The Lambda function's implementation should be in the <code>main.py</code> file. Below is an example showcasing our simple Hello World function:</p> functions/hello_world/main.py<pre><code>import json\nfrom dataclasses import dataclass\n\n@dataclass\nclass Input:\n    pass\n\n@dataclass\nclass Output:\n    message: str\n\ndef lambda_handler(event, context):\n\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"Hello World!\"})\n    }\n</code></pre> <p>The <code>Input</code> and <code>Output</code> data classes are the entrypoint for the documentation creation process. However, we will temporarily skip the docs generation details as this will be covered on a dedicated session.</p> <p>Moving forward, we've successfully implemented a straightforward lambda function that outputs a basic JSON response: <code>{\"message\": \"Hello World!\"}</code>.</p>"},{"location":"home/creating-a-hello-world/#the-lambda-stack-class","title":"The Lambda Stack Class","text":"<p>The Lambda Stack class handles the deployment of Lambda Functions. To deploy a function, you must instantiate its config class within the Lambda Stack class.</p> <p>Note</p> <p>     When you run the forge function command, the config classes are automatically integrated here for you. </p> <p>The Config classes require the Services Class to interact with AWS resources. Although we'll discuss AWS Services in detail later, for now, note that in the <code>infra/services/__init__.py</code> file, you'll find a Services class that acts as a factory for all AWS services.</p> infra/services/__init__.py<pre><code>from infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\n\nclass Services:\n\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n</code></pre> <p>The Lambda Stack automatically injects the Services class into the Config classes of each Lambda:</p> infra/stacks/lambda_stack.py<pre><code>from aws_cdk import Stack\nfrom constructs import Construct\nfrom infra.services import Services\nfrom functions.hello_world.config import HelloWorldConfig\n\n\nclass LambdaStack(Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n\n        super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs)\n\n        self.services = Services(self, context)\n\n        # HelloWorld\n        HelloWorldConfig(self.services)\n</code></pre>"},{"location":"home/creating-a-hello-world/#configuring-the-lambda-function","title":"Configuring The Lambda Function","text":"<p>In our Lambda Forge projects, the <code>config.py</code> file plays a crucial role in defining and configuring the dependencies required by a Lambda function.</p> <p>The Services class is automatically injected in the Config class of all Lambdas autoamtically by the Lambda Stack class that  functions/hello_world/config.py<pre><code>from infra.services import Services\n\nclass HelloWorldConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"HelloWorld\",\n            path=\"./functions/hello_world\",\n            description=\"A simple hello world\"\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/hello_world\", function, public=True)\n</code></pre></p> <p>The Forge CLI has significantly simplified the setup by automatically tailoring the function to meet our specifications. Essentially, the <code>config.py</code> file configures a Lambda Function to be named as <code>HelloWorld</code> accompanied by the description <code>A simple hello world</code>.</p> <p>Additionally, it sets up the function to respond to GET requests at the <code>/hello_world</code> path and designates it as a public endpoint, making it accessible without authentication.</p>"},{"location":"home/creating-a-hello-world/#pushing-the-code-to-github","title":"Pushing the Code to Github","text":"<p>With all the required settings now in place, we're ready to upload our code to the GitHub repository.</p> <p>Lambda Forge is designed to support a multi-stage deployment process, automatically creating environments for Production, Staging and Development. These environments correspond to the <code>main</code>, <code>staging</code>, and <code>dev</code> branches, respectively.</p> <p>For the sake of simplicity, we'll focus on deploying only the development branch at this moment, deferring the discussion on setting up a multi-stage environment to a future session.</p> <pre><code># Initialize the Git repository\ngit init\ngit add .\n\n# Commit the changes\ngit commit -m \"Initial commit\"\n\n# Set the remote repository\ngit remote add origin git@github.com:$GITHUB_USER/$GITHUB_REPO.git\n\n# Create, checkout, and push the 'dev' branch\ngit checkout -b dev\ngit push -u origin dev\n</code></pre>"},{"location":"home/creating-a-hello-world/#deploying-the-stacks","title":"Deploying the Stacks","text":"<p>Deploy the Dev Stack by running the following command in your terminal:</p> <pre><code>forge deploy --stack Dev\n</code></pre> <p>Lambda Forge ensures that every resource it creates on AWS follows a naming convention that integrates the deployment stage, the project name, and the resource name. This approach guarantees a consistent and clear identification methodology throughout the project.</p> <p>The project name is defined within the <code>cdk.json</code> file, linking each resource directly to its associated project and stage for easy management and recognition.</p> cdk.json<pre><code>    \"region\": \"$AWS-REGION\",\n    \"account\": \"$AWS-ACCOUNT\",\n    \"name\": \"Lambda-Forge-Demo\",\n    \"repo\": {\n      \"owner\": \"$GITHUB-OWNER\",\n      \"name\": \"$GITHUB-REPO\"\n    },\n</code></pre> <p>Following a successful deployment, a new pipeline will be created with the name <code>Dev-Lambda-Forge-Demo-Pipeline</code>. Access your AWS CodePipeline console to view it.</p> <p></p> <p>In a dedicated session, we'll delve into the specifics of the pipelines generated, including a closer examination of the development pipeline.</p> <p>After the pipeline execution concludes, proceed to your AWS Lambda console and locate the <code>Dev-Lambda-Forge-Demo-HelloWorld</code> function.</p> <p></p> <p>Select the function, then navigate to <code>Configurations -&gt; Triggers</code>. Here, you will be presented with a link to your newly deployed Lambda function, ready for use.</p> <p></p> <p>For this tutorial, the Lambda function is accessible via the following URL:</p> <ul> <li>https://tbd4it3lph.execute-api.us-east-2.amazonaws.com/dev/hello_world</li> </ul> <p>Congratulations! \ud83c\udf89 You've successfully deployed your very first Hello World function using Lambda Forge! \ud83d\ude80</p>"},{"location":"home/custom-codepipeline-steps/","title":"Tailoring AWS CodePipeline with CodeBuild Steps","text":"<p>AWS CodePipeline is a continuous integration and continuous deployment (CI/CD) tool that automates the steps required to release a new version of software. It connects directly to a GitHub repository, or other version control systems, to monitor for any changes in the codebase. Upon detecting changes, the pipeline automatically triggers a series of predefined steps that can include building the code, running tests, and deploying the code to various environments.</p> <p>The CodeBuild step within the pipeline is crucial, since it is the component that actually compiles the source code, runs tests, and produces ready-to-deploy software packages. Each step ensures that only code that passes all defined quality checks and tests is moved forward in the pipeline, leading towards deployment.</p>"},{"location":"home/custom-codepipeline-steps/#aws-codebuild","title":"AWS CodeBuild","text":"<p>In AWS CodeBuild, you have the flexibility to choose from a variety of setup options for your build environment. You can opt for AWS-managed images that come pre-equipped with tools for popular development environments, utilize a custom Docker image hosted on Amazon ECR or any other registry to tailor your environment to specific requirements, or precisely orchestrate your build steps through a <code>buildspec.yml</code> file. Each method is designed to cater to diverse project needs, offering everything from convenience to extensive customization, with Docker images providing the greatest scope for personalization.</p> <p>To simplify setting up CodeBuild projects, we've prepared a public Docker image hosted on ECR. This image is pre-configured with all the necessary tools and includes custom Python scripts for validation and docs generation, streamlining your workflow. You can access our optimized build environment here: https://gallery.ecr.aws/x8r4y7j7/lambda-forge.</p> <p>For those interested in customizing further, you're encouraged to use this image as a foundation to build your own project-specific private image. The necessary steps and configurations can be found in the Dockerfile provided below.</p> Dockerfile<pre><code>FROM python:3.9-slim\n\nWORKDIR /lambda-forge\n\nCOPY . /lambda-forge\n\n# Install nvm with Node.js and npm\nENV NODE_VERSION=16.13.0\nRUN apt-get update \\\n  &amp;&amp; apt-get install -y curl jq \nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\nENV NVM_DIR=/root/.nvm\nRUN . \"$NVM_DIR/nvm.sh\" &amp;&amp; nvm install ${NODE_VERSION}\nRUN . \"$NVM_DIR/nvm.sh\" &amp;&amp; nvm use v${NODE_VERSION}\nRUN . \"$NVM_DIR/nvm.sh\" &amp;&amp; nvm alias default v${NODE_VERSION}\nENV PATH=\"/root/.nvm/versions/node/v${NODE_VERSION}/bin/:${PATH}\"\nRUN node --version\nRUN npm --version\n\n# Install Node.js dependencies\nRUN apt-get update &amp;&amp; apt-get install -y gnupg \\\n  &amp;&amp; curl -fsSL https://deb.nodesource.com/setup_14.x | bash - \\\n  &amp;&amp; apt-get install -y nodejs graphviz \\\n  &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* \\ \n  &amp;&amp; npm install -g aws-cdk redoc-cli\n\n# Install Python dependencies\nRUN pip install --upgrade pip \\\n  &amp;&amp; pip install pyyaml pytest-html coverage awscli boto3==1.33.2 botocore==1.33.2 \\ \n  &amp;&amp; pip install -r base-requirements.txt \\ \n  &amp;&amp; pip install lambda-forge \n</code></pre>"},{"location":"home/custom-codepipeline-steps/#default-steps","title":"Default Steps","text":"<p>As previously mentioned, the public image we used to create the container includes some python scripts that can be utilized as steps in your local projects. Whether you choose to use them is entirely up to you. Each step is defined in the <code>infra/steps/__init__.py</code> file.</p> <p>The table below exemplifies the functionality of each step.</p> Step Description UnitTests Run the unit tests and fails if any test is broken Coverage Measure the coverage of the application and fails if it is below the theshold defined on the cdk.json file ValidateDocs Validates if a main.py file is missing the Input/Output dataclasses necessary to generate docs ValidateIntegrationTests Ensure that each API Gateway endpoint has at least one integration test decorated with pytest.mark.integration IntegrationTests Run the integration tests and fails if any test is broken Swagger Document the Api Gateway endpoints with Swagger based on the Input/Output dataclasses and deploy to the S3 bucket defined on the cdk.json file Redoc Document the Api Gateway endpoints with Redoc based on the Input/Output dataclasses and deploy to the S3 bucket defined on the cdk.json file Diagram Generates the AWS Architecture Diagram and deploy to the S3 bucket defined on the cdk.json file Wikis Generate Wiki pages and deploy to the S3 bucket defined on the cdk.json file TestReport Generate a test report and deploy to the S3 bucket defined on the cdk.json file CoverageReport Generate a coverage report and deploy to the S3 bucket defined on the cdk.json file"},{"location":"home/custom-codepipeline-steps/#custom-steps","title":"Custom Steps","text":"<p>Lambda Forge enables you to design custom pipeline steps, allowing you to create proprietary scripts for validating your specific requirements.</p> <p>For example, let's craft a simple custom script that traverses each file listed in the <code>functions.json</code> file. This script will trigger an error if it detects a <code># TODO</code> comment, halting the pipeline to ensure that no pending tasks are overlooked.</p> <p>Let's create a new file at <code>infra/scripts/validate_todo.py</code>:</p> infra/scripts/validate_todo.py<pre><code>import json\nimport os\n\ndef check_functions_for_todo(json_file):\n\n    functions = json.load(open(json_file, 'r'))\n\n    for function in functions:\n        path = function.get(\"path\", \"\")\n        with open(path, 'r') as file:\n            if '# TODO' in file.read():\n                raise ValueError(f\"TODO found in file: {path}\")\n\n    print(\"No TODO found in any files.\")\n\nif __name__ == \"__main__\":\n    json_file_path = \"functions.json\"\n    check_functions_for_todo(json_file_path)\n</code></pre> <p>In the upcoming section, we'll dive into the seamless integration of these steps into our multi-stage pipelines, unlocking a world of possibilities for streamlining our deployment processes.</p>"},{"location":"home/docs-generation/","title":"Generating Docs","text":"<p>Lambda Forge excels not only in structuring Lambda functions for scalability but also in facilitating the automatic documentation of key project components. It simplifies the deployment of documents to the cloud, making them readily accessible via API Gateway.</p> <p>Documentation is seamlessly generated through CodePipeline and is then deployed to the S3 Bucket defined within your <code>cdk.json</code> file. This automated process ensures your project's documentation is always up-to-date and easily accessible.</p>"},{"location":"home/docs-generation/#available-docs","title":"Available Docs","text":"<p>The table below lists the documentation generated by this tutorial, accessible via API Gateway endpoints.</p> Swagger Redoc Diagram Coverage Tests Wiki"},{"location":"home/docs-generation/#api-documentation-with-swagger-and-redoc","title":"API Documentation with Swagger and ReDoc","text":"<p>To automatically document your api endpoints with Swagger and Redoc, you should include the <code>Input</code> and <code>Output</code> dataclasses inside your <code>main.py</code> files. Case you have an endpoint that's expecting a path parameter, you can also include it in the <code>Path</code> dataclass.</p> <p>The code snippet below demonstrates all the types of data you can expect to work with, including simple data types, lists, custom objects, optional fields, and literal types, offering a clear understanding of the input and output contracts for the API.</p> main.py<pre><code>from dataclasses import dataclass\nfrom typing import List, Optional, Literal\n\n@dataclass\nclass Path:\n    id: str\n\n@dataclass\nclass Object:\n    a_string: str\n    an_int: int\n\n@dataclass\nclass Input:\n    a_string: str  \n    an_int: int  \n    a_boolean: bool  \n    a_list: List[str]  \n    an_object: Object  \n    a_list_of_object: List[Object]  \n    a_literal: Literal[\"a\", \"b\", \"c\"]  \n    an_optional: Optional[str]  \n\n\n@dataclass\nclass Output:\n    pass \n\ndef lambda_handler(event, context):\n\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"Hello Docs!\"})\n    }\n</code></pre>"},{"location":"home/docs-generation/#diagram","title":"Diagram","text":"<p>To view the current architecture diagram of your application, run the following command in your terminal:</p> <pre><code>forge diagram\n</code></pre> <p>This command generates a PNG file depicting your current architecture, including all triggers and invocations, as shown below:</p> <p> </p>"},{"location":"home/docs-generation/#setting-up-the-endpoints","title":"Setting Up the Endpoints","text":"<p>The endpoints to visualize your docs are configured within <code>docs/config.py</code>. This configuration file allows you to specify the types of documents to be displayed and their presentation locations.</p> <p>Similar to functions, authorizers can be implemented to safeguard the access to your project's documentation, maintaining its confidentiality and integrity.</p> docs/config.py<pre><code>from infra.services import Services\n\nclass DocsConfig:\n    def __init__(self, services: Services) -&gt; None:\n        # Public Swagger at /swagger\n        services.api_gateway.create_docs(endpoint=\"/swagger\", artifact=\"swagger\", public=True)\n\n        # Private Swagger at /swagger\n        services.api_gateway.create_docs(endpoint=\"/private/swagger\", artifact=\"swagger\", authorizer=\"secret\")\n\n        # Redoc at /redoc\n        services.api_gateway.create_docs(endpoint=\"/redoc\", artifact=\"redoc\", public=True)\n\n        # Architecture Diagram at /diagram\n        services.api_gateway.create_docs(endpoint=\"/diagram\", artifact=\"diagram\", public=True)\n\n        # Tests Report at /tests\n        services.api_gateway.create_docs(endpoint=\"/tests\", artifact=\"tests\", public=True)\n\n        # Coverage Report at /coverage\n        services.api_gateway.create_docs(endpoint=\"/coverage\", artifact=\"coverage\", public=True)\n\n        # Wiki at /wiki\n        # Use the Wiki's title as artifact\n        services.api_gateway.create_docs(endpoint=\"/wiki\", artifact=\"Wiki\", public=True)\n</code></pre>"},{"location":"home/getting-started/","title":"Getting Started","text":""},{"location":"home/getting-started/#install-and-configure-aws-cdk","title":"Install and Configure AWS CDK","text":"<p>Lambda Forge is built on top of AWS Cloud Development Kit (CDK) and it's essential for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. Execute the following commands to install the AWS CDK globally and set up your AWS credentials:</p> <pre><code>npm install -g aws-cdk\naws configure\ncdk bootstrap\n</code></pre> <p>During the configuration, you will be prompted to enter your AWS Access Key ID, Secret Access Key, default region name, and output format.</p>"},{"location":"home/getting-started/#create-a-github-personal-access-token","title":"Create a GitHub Personal Access Token","text":"<p>Lambda Forge uses CodePipeline to interact with your GitHub repository. To enable this, generate a GitHub personal access token by following these steps:</p> <ol> <li>Navigate to \"Developer Settings\" in your GitHub account.</li> <li>Select \"Personal access tokens,\" then \"Tokens (classic).\"</li> <li>Click \"Generate new token,\" ensuring the \"repo\" scope is selected for full control of private repositories.</li> <li>Complete the token generation process.</li> </ol> <p>You can find more informations about creating a GitHub Token here.</p> <p>Your token will follow this format: <code>ghp_********************************</code></p>"},{"location":"home/getting-started/#store-the-token-on-aws-secrets-manager","title":"Store the Token on AWS Secrets Manager","text":"<p>Save this token in AWS Secrets Manager as <code>plain text</code> using the exact name github-token. This specific naming is vital as it corresponds to the default identifier that the CDK looks for within your AWS account.</p>"},{"location":"home/getting-started/#create-a-new-s3-bucket","title":"Create a new S3 Bucket","text":"<p>Create a new S3 bucket dedicated to storing documentation artifacts that are going to be automatically generated by the pipeline.</p>"},{"location":"home/getting-started/#create-a-new-directory","title":"Create a New Directory","text":"<pre><code>mkdir lambda_forge_demo\ncd lambda_forge_demo\n</code></pre>"},{"location":"home/getting-started/#create-a-new-virtual-environment","title":"Create a New Virtual Environment","text":"<pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"home/getting-started/#install-lambda-forge","title":"Install lambda-forge","text":"<pre><code>pip install lambda-forge\n</code></pre>"},{"location":"home/getting-started/#forge-cli","title":"Forge CLI","text":"<p>The Forge Command Line Interface (CLI) is a powerful, versatile tool designed to streamline the development, deployment, and management of applications. It enables developers to automate repetitive tasks, manage project configurations, and interact directly with the services and infrastructure without leaving the terminal. This CLI tool simplifies complex processes through straightforward commands, significantly reducing development time and effort.</p>"},{"location":"home/getting-started/#verify-installation","title":"Verify Installation","text":"<p>Having successfully installed Lambda Forge, you are now ready to explore the capabilities of the Forge CLI. Begin by entering the following command to access the comprehensive list of available options and commands:</p> <pre><code>forge --help\n</code></pre> <p>Here's a concise list of the commands supported by Forge:</p> <pre><code>Commands:\n  authorizer  Generates an authorizer for AWS Lambda functions.\n  deploy      Deploys a stack to AWS\n  diagram     Create a diagram of the project in png format\n  doc         Creates a new doc template for the project.\n  function    Creates a Lambda function with a predefined structure and...\n  layer       Creates and installs a new Lambda layer.\n  live        Starts a live development environment for the specified...\n  output      List the outputs of the stacks on AWS CloudFormation\n  project     Initializes a new AWS Lambda project with a specified...\n  service     Scaffolds the structure for a specified AWS service...\n  test        Run the tests or coverage of the project\n</code></pre> <p>For a comprehensive list of configurations that each Forge command supports, you can refer to the command line help by running:</p> <p><code>forge $COMMAND --help</code>.</p> <p>Later in this tutorial, we'll delve into the specifics of each command. But for now, let's kickstart by establishing the foundation of our project.</p>"},{"location":"home/getting-started/#create-a-new-project","title":"Create a New Project","text":"<p>Start a new project named <code>lambda-forge-demo</code>.</p> <pre><code>forge project --name lambda-forge-demo --repo-owner \"$GITHUB-OWNER\" --repo-name \"$GITHUB-REPO\" --bucket \"$S3-BUCKET\" --account \"$AWS-ACCOUNT\"\n</code></pre> <p>Make sure to replace <code>$GITHUB-OWNER</code> and <code>$GITHUB-REPO</code> with the actual GitHub owner and the name of an empty repository and the <code>$S3-BUCKET</code> with the name of a S3 bucket and the <code>$AWS-ACCOUNT</code> with your AWS Account ID.</p> <p>Alternatively, you can also run <code>forge project</code> to create the project in interactive mode.</p>"},{"location":"home/getting-started/#project-structure","title":"Project Structure","text":"<p>Upon creatig your project, some directories and files are automatically generated for you. This initial structure is designed to streamline the setup process and provide a solid foundation for further development.</p> <p>In the upcoming sections of this tutorial, we'll explore each of these components in detail. For now, familiarize yourself with the foundational structure that should resemble the following:</p> <pre><code>.\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 config.py\n\u2502\n\u251c\u2500\u2500 infra\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 services\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 aws_lambda.py\n\u2502   \u2502   \u2514\u2500\u2500 api_gateway.py\n\u2502   \u251c\u2500\u2500 stacks\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 dev_stack.py\n\u2502   \u2502   \u251c\u2500\u2500 lambda_stack.py\n\u2502   \u2502   \u251c\u2500\u2500 prod_stack.py\n\u2502   \u2502   \u2514\u2500\u2500 staging_stack.py\n\u2502   \u2514\u2500\u2500 stages\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 deploy.py\n\u2502\n\u251c\u2500\u2500 .coveragerc\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 app.py\n\u251c\u2500\u2500 cdk.json\n\u251c\u2500\u2500 pytest.ini\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>The <code>cdk.json</code> file, located at the root of your directory, serves as the central configuration hub for Lambda Forge projects. When you run the <code>forge project</code> command, Forge automatically applies the informed settings into the cdk.json file.</p> cdk.json<pre><code>    \"region\": \"$AWS-REGION\",\n    \"account\": \"$AWS-ACCOUNT\",\n    \"name\": \"Lambda-Forge-Demo\",\n    \"repo\": {\n      \"owner\": \"$GITHUB-OWNER\",\n      \"name\": \"$GITHUB-REPO\"\n    },\n    \"bucket\": \"$S3-BUCKET\",\n</code></pre>"},{"location":"home/lambda-layers/","title":"Utilizing Lambda Layers for Code Reuse and External Library Integration","text":""},{"location":"home/lambda-layers/#what-are-lambda-layers","title":"What Are Lambda Layers?","text":"<p>Lambda Layers are essentially ZIP archives containing libraries, custom runtime environments, or other dependencies. You can include these layers in your Lambda function\u2019s execution environment without having to bundle them directly with your function's deployment package. This means you can use libraries or custom runtimes across multiple Lambda functions without needing to include them in each function\u2019s codebase.</p>"},{"location":"home/lambda-layers/#how-they-work","title":"How They Work","text":"<p>When you create a Lambda function, you specify which layers to include in its execution environment. During execution, AWS Lambda configures the function's environment to include the content of the specified layers. This content is available to your function's code just as if it were included in the deployment package directly.</p>"},{"location":"home/lambda-layers/#use-cases","title":"Use Cases","text":"<ul> <li>Sharing code: Commonly used code can be placed in a layer and shared among multiple functions.</li> <li>Custom runtimes: You can use layers to deploy functions in languages that AWS Lambda does not natively support by including the necessary runtime in a layer.</li> <li>Configuration files: Layers can be used to store configuration files that multiple functions need to access.</li> </ul>"},{"location":"home/lambda-layers/#aws-lambda-development-with-custom-layers","title":"AWS Lambda Development with Custom Layers","text":"<p>Forge streamlines the process of creating and sharing custom layers across AWS Lambda functions, significantly simplifying code reuse and management. This section walks you through creating a custom layer using Forge, integrating it into your development workflow, and utilizing it within a Lambda function.</p>"},{"location":"home/lambda-layers/#creating-a-custom-layer","title":"Creating a Custom Layer","text":"<p>To begin, execute the following command to create a custom layer named <code>my_custom_layer</code>:</p> <pre><code>forge layer --custom my_custom_layer\n</code></pre> <p>This command sets up a specific directory structure for your layer within your project, organizing the code efficiently:</p> <pre><code>layers\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 my_custom_layer\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 my_custom_layer.py\n</code></pre> <p>Forge not only initializes the necessary structure but also populates my_custom_layer.py with a starter function. This function acts as a blueprint for your shared code:</p> layers/my_custom_layer/my_custom_layer.py<pre><code>def hello_from_layer():\n    return \"Hello from my_custom_layer layer!\"\n</code></pre> <p>Additionally, Forge sets the new custom layer in the Layers class.</p> infra/services/layers.py<pre><code>from aws_cdk import aws_lambda as _lambda\nfrom lambda_forge.path import Path\n\n\nclass Layers:\n    def __init__(self, scope) -&gt; None:\n\n        self.my_custom_layer = _lambda.LayerVersion(\n            scope,\n            id='MyCustomLayer',\n            code=_lambda.Code.from_asset(Path.layer('layers/my_custom_layer')),\n            compatible_runtimes=[_lambda.Runtime.PYTHON_3_9],\n            description='',\n         )\n</code></pre> <p>Traditionally, working with Lambda layers introduces complexity during development. Since Lambda layers are deployed as zip files and run within the Lambda execution environment, developers usually face challenges in utilizing these layers locally. This often leads to a disconnect between development and production environments, complicating the development process.</p> <p>When you create a custom layer using Forge, the new layer is automatically integrated into your local virtual environment, similar to installing an external library from pip. However, to ensure that these changes are fully recognized, you may need to reload your IDE or reselect your virtual environment.</p> <p>Note</p> In case you need to reinstall the custom layers into your virtual environment, use the command:  <pre><code>forge layer --install\n</code></pre>"},{"location":"home/lambda-layers/#creating-a-lambda-function-utilizing-the-custom-layer","title":"Creating a Lambda Function Utilizing the Custom Layer","text":"<p>Create a new Lambda function that leverages your custom layer by running:</p> <pre><code>forge function custom --method \"GET\" --description \"A function that uses my_custom_layer\" --public\n</code></pre> <p>This command simply creates a public function named <code>custom</code> inside the <code>functions</code> directory.</p> <pre><code>functions\n\u2514\u2500\u2500 custom\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 integration.py\n    \u251c\u2500\u2500 main.py\n    \u2514\u2500\u2500 unit.py\n</code></pre> <p>Now, implement the function to utilize the custom layer:</p> functions/layers/custom/main.py<pre><code>import json\nfrom dataclasses import dataclass\n\nimport my_custom_layer\n\n\n@dataclass\nclass Input:\n    pass\n\n\n@dataclass\nclass Output:\n    message: str\n\n\ndef lambda_handler(event, context):\n\n    message = my_custom_layer.hello_from_layer()\n\n    return {\"statusCode\": 200, \"body\": json.dumps({\"message\": message})}\n</code></pre> <p>Finally, configure the function to make use of the <code>my_custom_layer</code> layer:</p> functions/layers/custom/config.py<pre><code>from infra.services import Services\n\n\nclass CustomConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Custom\",\n            path=\"./functions/custom\",\n            description=\"A function to make use of the custom layer\",\n            layers=[services.layers.my_custom_layer],\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/custom\", function, public=True)\n</code></pre> <p>Once you've committed and pushed your code to GitHub and the pipeline has successfully executed, making a GET request to the generated URL should return the following response:</p> <pre><code>{\n  \"message\": \"Hello from my_custom_layer layer!\"\n}\n</code></pre> <p>The URL for this tutorial is:</p> <ul> <li>https://tbd4it3lph.execute-api.us-east-2.amazonaws.com/dev/custom</li> </ul>"},{"location":"home/lambda-layers/#aws-lambda-development-with-external-libraries","title":"AWS Lambda Development with External Libraries","text":"<p>In software development, using external libraries is a common practice to extend functionality and streamline the development process. When working with AWS Lambda, incorporating these external libraries requires integrating them as layers into our Lambda functions.</p> <p>To illustrate this scenario, we will develop a new lambda function aimed to parsing the data retrieved from the external API https://randomuser.me/api/, a public service for generating random fake user data. Since the <code>requests</code> library is not inherently included in Python, it will be necessary to integrate it as a layer in our lambda function.</p> <p>To create an external layer, run the following command in your terminal:</p> <pre><code>forge layer --external $LAYER\n</code></pre>"},{"location":"home/lambda-layers/#incorporating-requests-from-public-layers","title":"Incorporating Requests from Public Layers","text":"<p>To create a new layer for the requests library, simply run:</p> <pre><code>forge layer --external requests\n</code></pre> <p>Lambda Forge will automatically create and deploy this layer to AWS, additionally it updates the <code>Layer</code> class to utilize your newly created layer.</p> infra/services/layers.py<pre><code>class Layers:\n    def __init__(self, scope) -&gt; None:\n\n        self.my_custom_layer = _lambda.LayerVersion(\n            scope,\n            id='MyCustomLayer',\n            code=_lambda.Code.from_asset(Path.layer('layers/my_custom_layer')),\n            compatible_runtimes=[_lambda.Runtime.PYTHON_3_9],\n            description='',\n         )\n\n        self.requests_layer = _lambda.LayerVersion.from_layer_version_arn(\n            scope,\n            id='RequestsLayer',\n            layer_version_arn='arn:aws:lambda:us-east-2:211125768252:layer:requests:1',\n         )\n</code></pre>"},{"location":"home/lambda-layers/#creating-a-lambda-function-utilizing-the-requests-library","title":"Creating a Lambda Function Utilizing the Requests Library","text":"<p>To create a Lambda function that leverages the Requests library, execute the following command:</p> <pre><code>forge function external --method \"GET\" --description \"A function that uses an external library\" --public\n</code></pre> <p>This command creates a new function named <code>external</code> inside the functions directory.</p> <pre><code>functions\n\u2514\u2500\u2500 external\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 integration.py\n    \u251c\u2500\u2500 main.py\n    \u2514\u2500\u2500 unit.py\n</code></pre> <p>Now, implement the function to utilize the custom layer:</p> <pre><code>import json\nfrom dataclasses import dataclass\n\nimport requests\n\n\n@dataclass\nclass Input:\n    pass\n\n\n@dataclass\nclass Name:\n    title: str\n    first: str\n    last: str\n\n\n@dataclass\nclass Output:\n    name: Name\n    gender: str\n    email: str\n\n\ndef lambda_handler(event, context):\n\n    result = requests.get(\"https://randomuser.me/api\").json()[\"results\"][0]\n\n    data = {\n        \"name\": result[\"name\"],\n        \"gender\": result[\"gender\"],\n        \"email\": result[\"email\"],\n    }\n\n    return {\"statusCode\": 200, \"body\": json.dumps({\"data\": data})}\n</code></pre> <p>Additionally, update the unit tests to expect the correct output message:</p> functions/layers/external/unit.py<pre><code>import json\nfrom .main import lambda_handler\n\n\ndef test_lambda_handler():\n\n    response = lambda_handler(None, None)\n    body = json.loads(response[\"body\"])\n\n    assert [\"name\", \"gender\", \"email\"] == list(body.keys())\n</code></pre> <p>Finally, configure the function to make use of the requests layer:</p> functions/layers/custom/config.py<pre><code>from infra.services import Services\n\n\nclass ExternalConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"External\",\n            path=\"./functions/external\",\n            description=\"A function that uses an external library\",\n            layers=[services.layers.requests_layer],\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/external\", function, public=True)\n</code></pre> <p>Once you've committed and pushed your code to GitHub and the pipeline has successfully executed, making a GET request to the generated URL should return the following response:</p> <pre><code>{\n  \"name\": {\n    \"title\": \"str\",\n    \"first\": \"str\",\n    \"last\": \"str\"\n  },\n  \"gender\": \"str\",\n  \"email\": \"str\"\n}\n</code></pre> <p>For this tutorial, the generated URL is:</p> <ul> <li>https://tbd4it3lph.execute-api.us-east-2.amazonaws.com/dev/external</li> </ul>"},{"location":"home/live-development/","title":"Synchronizing Local Changes with AWS in Real-Time","text":"<p>Working with AWS Lambda introduces unique challenges compared to traditional server-based setups. Lambda functions are ephemeral and event-driven, terminating immediately after their tasks are completed. This nature complicates local development and testing.</p> <p>To mitigate these issues, AWS developed the Serverless Application Model (SAM), which facilitates the deployment of serverless applications with YAML configurations and supports local emulation via the SAM CLI. However, SAM faces difficulties with complex applications, presents a steep learning curve, and its local testing features often fall short of replicating the complete AWS environment, potentially leading to inconsistencies.</p> <p>To improve the development experience, Lambda Forge introduces a different solution. Instead of trying to locally emulate Lambda functions, Lambda Forge deploys a stub function to AWS and utilizes the MQTT over Websockets protocol to establish a connection to a local server running on the developer's machine. This configuration proxies requests intended for the AWS Lambda function directly to the local environment, thereby facilitating an accurate replication of cloud conditions and enabling seamless local development.</p>"},{"location":"home/live-development/#live-server","title":"Live Server","text":"<p>Starting the live development server with Lambda Forge is straightforward. Execute the following command in your terminal:</p> <pre><code>forge live server\n</code></pre> <p>Upon executing this command, Lambda Forge will initiate a local <code>MQTT/Websockets</code> server and deploy a stub Lambda function for every Lambda defined in your Lambda Stack. These functions establish a connection with the local server, allowing requests sent to the cloud to be proxied to the developer's machine.</p> <p>Lambda Forge also automatically configures triggers defined in the <code>functions.json</code> and creates a live development version for each trigger.</p> <p></p> <p>As illustrated in the image above, Lambda Forge sets up real AWS endpoints for live development. It also creates an SNS topic to trigger the Hello World function, which can be activated by multiple sources as previously defined in the AWS Services documentation.</p> <p>By calling these endpoints or the SNS topic, the requests sent to the cloud will be proxied to your local machine. For instance, when sending requests to the <code>Hello World</code> function and modifying its responses, you can observe live reloading during development. </p> <p></p>"},{"location":"home/live-development/#live-logs","title":"Live Logs","text":"<p>During live development sessions, all events are automatically logged. To view these logs, use the following command:</p> <pre><code>forge live logs\n</code></pre> <p>Executing this command saves the logs and starts a live tailing process, enabling you to monitor them in real time. You can run this command in a separate terminal while the live server is running. </p> <p>Note</p> <p> Print statements from the Lambda functions are captured and logged as well. </p> <p> </p>"},{"location":"home/live-development/#live-trigger","title":"Live Trigger","text":"<p>Unlike the API Gateway, which allows for straightforward testing via browsers or tools like Postman and Insomnia, alternative triggers such as SNS, SQS and S3 are not as readily accessible. These triggers generally necessitate the use of custom code or involve manual setup via the AWS Management Console.</p> <p>To simplify the testing of these alternative triggers, Lambda Forge offers a streamlined solution. This feature allows developers to publish messages directly to the configured resources, making the testing process as simple and efficient as with API Gateway.</p> <p>To trigger a service, use the following command:</p> <pre><code>forge live trigger\n</code></pre> <p>Executing this command initiates a session in your terminal, enabling you to publish messages directly to AWS resources such as SQS, SNS, or upload files to an S3 bucket.</p> <p> </p> <p>You can use this command to trigger your live resources seamlessly, and it can be run in different terminal tabs for a smooth live development session.</p>"},{"location":"home/multi-stage-environments/","title":"Multi-Stage Environments With AWS CodePipeline","text":"<p>In practical scenarios, it is highly recommended to adopt a multi-stage development approach. This strategy allows you to freely develop and test your code in isolated environments without affecting your live production environment and, consequently, the real-world users of your application.</p> <p>In Lambda Forge, the pipelines for development, staging, and production are meticulously organized within distinct files, found at <code>infra/stacks/dev_stack.py</code>, <code>infra/stacks/staging_stack.py</code>, and <code>infra/stacks/prod_stack.py</code>, respectively.</p> <p>Each stage is designed to operate with its own set of isolated resources, to ensure that changes in one environment do not inadvertently affect another.</p> <p>Note</p> <p>Lambda Forge provides a suggested pipeline configuration for each stage of deployment. You're encouraged to customize these pipelines to fit your project's needs. Whether adding new steps, adjusting existing ones, reordering or even removing some of them. </p>"},{"location":"home/multi-stage-environments/#development-environment","title":"Development Environment","text":"<p>The <code>Development</code> environment is where the initial coding and feature implementation occur, allowing developers to make frequent changes and test new ideas in an isolated environment.</p> <p>This environment is strategically structured to facilitate rapid deployments, allowing new features to be rolled out directly without undergoing any preliminary validation steps. It functions essentially as a sandbox environment, providing developers with a space to both develop and test new features in a fast-paced and flexible setting. This approach enables immediate feedback and iterative improvements, streamlining the development process.</p>"},{"location":"home/multi-stage-environments/#development-environment-configuration","title":"Development Environment Configuration","text":"<p>This section details the setup process for the development environment.</p> infra/stacks/dev_stack.py<pre><code>import aws_cdk as cdk\nfrom aws_cdk import aws_codebuild as codebuild\nfrom aws_cdk import pipelines as pipelines\nfrom aws_cdk.pipelines import CodePipelineSource\nfrom constructs import Construct\nfrom infra.stages.deploy import DeployStage\n\nfrom lambda_forge.constants import ECR\nfrom lambda_forge.context import context\nfrom lambda_forge.steps import CodeBuildSteps\n\n\n@context(stage=\"Dev\", resources=\"dev\")\nclass DevStack(cdk.Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n        super().__init__(scope, context.create_id(\"Stack\"), **kwargs)\n\n        source = CodePipelineSource.git_hub(f\"{context.repo['owner']}/{context.repo['name']}\", \"dev\")\n\n        pipeline = pipelines.CodePipeline(\n            self,\n            \"Pipeline\",\n            pipeline_name=context.create_id(\"Pipeline\"),\n            synth=pipelines.ShellStep(\"Synth\", input=source, commands=[\"cdk synth\"]),\n            code_build_defaults=pipelines.CodeBuildOptions(\n                build_environment=codebuild.BuildEnvironment(\n                    build_image=codebuild.LinuxBuildImage.from_docker_registry(ECR.LATEST),\n                )\n            ),\n        )\n\n        steps = CodeBuildSteps(self, context, source=source)\n\n        # post\n        swagger = steps.swagger()\n        redoc = steps.redoc()\n\n        pipeline.add_stage(DeployStage(self, context), post=[swagger, redoc])\n</code></pre> <p>On line 10, the <code>context</code> decorator assigns the stage name as <code>Dev</code> and configures the use of resources tagged as <code>dev</code> in the <code>cdk.json</code> file. Moreover, it imports some additional configuration variables from the <code>cdk.json</code> file, assigning them to the argument named <code>context</code>.</p> <p>Additionally, we incorporate the source code from the <code>dev</code> branch hosted on GitHub into the pipeline. Subsequently, we finalize the deployment of the Lambda functions by activating the <code>DeployStage</code>. Post-deployment, we generate <code>Swagger</code> and <code>Redoc</code> documentation using a helper class provided by Lambda Forge. This class utilizes the public Docker image hosted on ECR described in the previous chapter to facilitate these operations.</p>"},{"location":"home/multi-stage-environments/#dev-pipeline-workflow","title":"Dev Pipeline Workflow","text":"<p>The diagram below succinctly illustrates the default pipeline configuration for the <code>Dev</code> stage and established within the AWS CodePipeline.</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline     UpdatePipeline --&gt; Assets     Assets --&gt; Deployment     Deployment --&gt; Redoc     Deployment --&gt; Swagger"},{"location":"home/multi-stage-environments/#adding-wikis","title":"Adding Wikis","text":"<p>For project success through teamwork, prioritizing knowledge sharing is key. Leveraging wikis facilitates seamless information dissemination, ensuring everyone is on the same page.</p> <p>First, let's create a markdown file that we'll use as our document.</p> <pre><code>docs/\n\u2514\u2500\u2500 wikis/\n    \u2514\u2500\u2500 wiki.md\n</code></pre> <p>Let's proceed by updating the pipeline to generate the S3 artifact containing our wikis.</p> infra/stacks/dev_stack.py<pre><code>        # post\n        redoc = steps.redoc()\n        swagger = steps.swagger()\n        wikis = [\n            {\n                \"title\": \"Wiki\",\n                \"file_path\": \"docs/wikis/wiki.md\",\n                \"favicon\": \"https://docs.lambda-forge.com/images/favicon.png\",\n            }\n        ]\n        wikis = steps.wikis(wikis)\n\n        pipeline.add_stage(\n            DeployStage(self, context),\n            post=[\n                redoc,\n                swagger,\n                wikis,\n            ]\n        )\n</code></pre> <p>Below is the diagram illustrating the updated pipeline configuration:</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline[Update Pipeline]     UpdatePipeline --&gt; Assets     Assets --&gt; Deployment     Deployment --&gt; Redoc     Deployment --&gt; Swagger     Deployment --&gt; Wikis"},{"location":"home/multi-stage-environments/#deploying-the-dev-environment","title":"Deploying the Dev Environment","text":"<p>After committing and pushing the code to Github, our pipeline should be triggered and updated with the latest configurations.</p> <p></p>"},{"location":"home/multi-stage-environments/#staging-environment","title":"Staging Environment","text":"<p>The <code>Staging</code> environment serves as a near-replica of the production environment, enabling thorough testing and quality assurance processes to catch any bugs or issues before they reach the end-users.</p>"},{"location":"home/multi-stage-environments/#staging-environment-configuration","title":"Staging Environment Configuration","text":"<p>Let's take a deeper look in the staging configuration file.</p> infra/stacks/staging_stack.py<pre><code>import aws_cdk as cdk\nfrom aws_cdk import aws_codebuild as codebuild\nfrom aws_cdk import pipelines as pipelines\nfrom aws_cdk.pipelines import CodePipelineSource\nfrom constructs import Construct\nfrom infra.stages.deploy import DeployStage\n\nfrom lambda_forge.constants import ECR\nfrom lambda_forge.context import context\nfrom lambda_forge.steps import CodeBuildSteps\n\n\n@context(stage=\"Staging\", resources=\"staging\")\nclass StagingStack(cdk.Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n        super().__init__(scope, context.create_id(\"Stack\"), **kwargs)\n\n        source = CodePipelineSource.git_hub(f\"{context.repo['owner']}/{context.repo['name']}\", \"staging\")\n\n        pipeline = pipelines.CodePipeline(\n            self,\n            \"Pipeline\",\n            pipeline_name=context.create_id(\"Pipeline\"),\n            synth=pipelines.ShellStep(\"Synth\", input=source, commands=[\"cdk synth\"]),\n            code_build_defaults=pipelines.CodeBuildOptions(\n                build_environment=codebuild.BuildEnvironment(\n                    build_image=codebuild.LinuxBuildImage.from_docker_registry(ECR.LATEST),\n                )\n            ),\n        )\n\n        steps = CodeBuildSteps(self, context, source=source)\n\n        # pre\n        unit_tests = steps.unit_tests()\n        coverage = steps.coverage()\n        validate_docs = steps.validate_docs()\n        validate_integration_tests = steps.validate_integration_tests()\n        validate_todo = steps.custom_step(\n            name=\"ValidateTodo\", commands=[\"python infra/scripts/validate_todo.py\"]\n        )\n\n        # post\n        redoc = steps.redoc()\n        swagger = steps.swagger()\n        integration_tests = steps.integration_tests()\n        tests_report = steps.tests_report()\n        coverage_report = steps.coverage_report()\n\n        pipeline.add_stage(\n            DeployStage(self, context),\n            pre=[\n                unit_tests, \n                coverage, \n                validate_integration_tests, \n                validate_docs, \n                validate_todo\n            ],\n            post=[\n                redoc,\n                swagger,\n                integration_tests,\n                tests_report,\n                coverage_report,\n            ],\n        )\n</code></pre> <p>Similar to the <code>Dev</code> environment, this environment is named <code>Staging</code>, with resources designated as <code>staging</code> in the <code>cdk.json</code> file. We also integrate the source code from the <code>staging</code> branch on GitHub into the pipeline. However, in contrast to Dev, the Staging environment incorporates stringent quality assurance protocols prior to deployment.</p>"},{"location":"home/multi-stage-environments/#staging-pipeline-workflow","title":"Staging Pipeline Workflow","text":"<p>The diagram below succinctly illustrates the default pipeline configuration for the <code>Staging</code> stage and established within the AWS CodePipeline.</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline     UpdatePipeline --&gt; Assets     Assets --&gt; UnitTests     Assets --&gt; Coverage     Assets --&gt; ValidateDocs     Assets --&gt; ValidateIntegrationTests     UnitTests --&gt; Deploy     Coverage --&gt; Deploy     ValidateDocs --&gt; Deploy     ValidateIntegrationTests --&gt; Deploy     Deploy --&gt; Redoc     Deploy --&gt; Swagger     Deploy --&gt; IntegrationTests     Deploy --&gt; TestsReport     Deploy --&gt; CoverageReport"},{"location":"home/multi-stage-environments/#adding-the-validatetodo-step","title":"Adding the ValidateTodo Step","text":"<p>Let's fine-tune our pipeline by incorporating the <code>ValidateTodo</code> step, which we devised during the previous session. This step will scan files for any TODO comments, promptly raising an error if one is detected.</p> infra/stacks/staging_stack.py<pre><code>        # pre\n        unit_tests = steps.unit_tests()\n        coverage = steps.coverage()\n        validate_docs = steps.validate_docs()\n        validate_integration_tests = steps.validate_integration_tests()\n        validate_todo = steps.custom_step(\n            name=\"ValidateTodo\",\n            commands=[\"python infra/scripts/validate_todo.py\"]\n        )\n\n        # post\n        redoc = steps.redoc()\n        swagger = steps.swagger()\n        integration_tests = steps.integration_tests()\n        tests_report = steps.tests_report()\n        coverage_report = steps.coverage_report()\n\n        pipeline.add_stage(\n            DeployStage(self, context),\n            pre=[\n                unit_tests,\n                coverage,\n                validate_docs,\n                validate_todo,\n                validate_integration_tests,\n            ],\n            post=[\n                redoc,\n                swagger,\n                integration_tests,\n                tests_report,\n                coverage_report,\n            ],\n        )\n</code></pre> <p>We've introduced the validate todo step prior to deployment. Below, you'll find the updated pipeline configuration.</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline     UpdatePipeline --&gt; Assets     Assets --&gt; UnitTests     Assets --&gt; Coverage     Assets --&gt; ValidateDocs     Assets --&gt; ValidateIntegrationTests     Assets --&gt; ValidateTodo     UnitTests --&gt; Deploy     Coverage --&gt; Deploy     ValidateDocs --&gt; Deploy     ValidateTodo --&gt; Deploy     ValidateIntegrationTests --&gt; Deploy     Deploy --&gt; CoverageReport     Deploy --&gt; IntegrationTests     Deploy --&gt; Redoc     Deploy --&gt; Swagger     Deploy --&gt; TestsReport"},{"location":"home/multi-stage-environments/#deploying-the-staging-environment","title":"Deploying the Staging Environment","text":"<p>Next, let's deploy the staging environment with CDK, adhering to the naming conventions established by Forge:</p> <pre><code>forge deploy --stack Staging\n</code></pre> <p>After some minutes, a new pipeline named <code>Staging-Lambda-Forge-Demo-Stack</code> is created on AWS CodePipeline.</p> <p></p> <p>In the staging environment, we conduct rigorous testing to ensure the robustness of our applications. Integration tests are particularly vital as they simulate real-world scenarios in an environment closely mirroring production.</p> <p>Let's take the <code>HelloWorld</code> integration test as example.</p> functions/hello_world/integration.py<pre><code>import pytest\nimport requests\nfrom lambda_forge.constants import BASE_URL\n\n\n@pytest.mark.integration(method=\"GET\", endpoint=\"/hello_world\")\ndef test_hello_world_status_code_is_200():\n\n    response = requests.get(url=f\"{BASE_URL}/hello_world\")\n\n    assert response.status_code == 200\n</code></pre> <p>Note that, this integration test is actually sending a <code>GET</code> request to the deployed staging <code>/hello_world</code> endpoint and expecting <code>200</code> as the status code.</p> <p>However, given this is our initial deployment, the BASE URL has not been established yet, leading the IntegrationTests step to fail.</p> <p></p>"},{"location":"home/multi-stage-environments/#resolving-the-initial-staging-deployment-error","title":"Resolving the Initial Staging Deployment Error","text":"<p>To address the initial staging deployment error, we need to configure the BASE URL specifically for the integration tests.</p> <p>First, run the following command to list all variables from your CloudFormation stacks:</p> <pre><code>forge output\n</code></pre> <p>Upon running the command, the variables will be listed in your terminal:</p> <pre><code>Stack Name: Dev-Lambda-Forge-Demo-Lambda-Stack\nBASE-URL: https://zo3691q3pd.execute-api.us-east-2.amazonaws.com/dev/\n\n\nStack Name: Staging-Lambda-Forge-Demo-Lambda-Stack\nBASE-URL: https://qkaer0f0q5.execute-api.us-east-2.amazonaws.com/staging/\n\n\nStack Name: Prod-Lambda-Forge-Demo-Lambda-Stack\nBASE-URL: https://byi76zqidj.execute-api.us-east-2.amazonaws.com/prod/\n</code></pre> <p>With the BASE URL identified, incorporate it into your <code>cdk.json</code> file under the <code>base_url</code> key. This adjustment ensures that all integration tests can interact seamlessly with the staging environment for automated testing.</p> cdk.json<pre><code>    \"base_url\": \"https://qkaer0f0q5.execute-api.us-east-2.amazonaws.com/staging/\"\n</code></pre> <p>Once the base URL is properly configured for the integration tests, commit your changes and push the updated code to GitHub. Following these adjustments, the pipeline should successfully complete its run.</p> <p></p>"},{"location":"home/multi-stage-environments/#production-environment","title":"Production Environment","text":"<p>The <code>Production</code> environment represents the phase where the tested and stable version of the software is deployed. This version is accessible to end-users and operates within the live environment. It is imperative that this stage remains the most safeguarded, permitting only fully vetted and secure code to be deployed. This precaution helps in minimizing the risk of exposing end-users to bugs or undesirable functionalities, ensuring a seamless and reliable user experience.</p>"},{"location":"home/multi-stage-environments/#production-environment-configuration","title":"Production Environment Configuration","text":"<pre><code>import aws_cdk as cdk\nfrom aws_cdk import aws_codebuild as codebuild\nfrom aws_cdk import pipelines\nfrom aws_cdk.pipelines import CodePipelineSource\nfrom constructs import Construct\nfrom infra.stages.deploy import DeployStage\n\nfrom lambda_forge.constants import ECR\nfrom lambda_forge.context import context\nfrom lambda_forge.steps import CodeBuildSteps\n\n\n@context(stage=\"Prod\", resources=\"prod\")\nclass ProdStack(cdk.Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n        super().__init__(scope, context.create_id(\"Stack\"), **kwargs)\n\n        source = CodePipelineSource.git_hub(f\"{context.repo['owner']}/{context.repo['name']}\", \"main\")\n\n        pipeline = pipelines.CodePipeline(\n            self,\n            \"Pipeline\",\n            pipeline_name=context.create_id(\"Pipeline\"),\n            synth=pipelines.ShellStep(\"Synth\", input=source, commands=[\"cdk synth\"]),\n            code_build_defaults=pipelines.CodeBuildOptions(\n                build_environment=codebuild.BuildEnvironment(\n                    build_image=codebuild.LinuxBuildImage.from_docker_registry(ECR.LATEST),\n                )\n            ),\n        )\n\n        steps = CodeBuildSteps(self, context, source=source)\n\n        # pre\n        unit_tests = steps.unit_tests()\n        integration_tests = steps.integration_tests()\n\n        # post\n        diagram = steps.diagram()\n        redoc = steps.redoc()\n        swagger = steps.swagger()\n\n        pipeline.add_stage(\n            DeployStage(self, context),\n            pre=[\n                unit_tests,\n                integration_tests,\n            ],\n            post=[\n                diagram,\n                redoc,\n                swagger,\n            ],\n        )\n</code></pre> <p>This environment is named <code>Prod</code> and the resources used are provenient from the <code>prod</code> key in the <code>cdk.json</code> file. Additionally, the <code>main</code> branch on GitHub is being used to trigger the pipeline.</p> <p>Considering the paramount importance of security and integrity in our production environment, we meticulously replicate tests and reinforce safeguards prior to deployment. This rigorous process guarantees that any modifications adhere to our stringent quality benchmarks, thereby safeguarding against vulnerabilities and upholding a seamless user experience. Following this, we proceed to generate the Swagger and Redoc docs alongside the AWS Architecture Diagram for our application.</p>"},{"location":"home/multi-stage-environments/#production-pipeline-workflow","title":"Production Pipeline Workflow","text":"<p>The following diagram visually represents the configuration established in AWS CodePipeline.</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline     UpdatePipeline --&gt; Assets     Assets --&gt; IntegrationTests     Assets --&gt; UnitTests     IntegrationTests --&gt; Deployment     UnitTests --&gt; Deployment     Deployment --&gt; Diagram     Deployment --&gt; Redoc     Deployment --&gt; Swagger"},{"location":"home/multi-stage-environments/#deploying-the-production-environment","title":"Deploying the Production Environment","text":"<p>Deploy your production environment using the AWS CDK, adhering to the naming conventions.</p> <pre><code>forge deploy --stack Prod\n</code></pre> <p>Executing this command initiates the creation of a new pipeline in AWS CodePipeline, designed to automate our deployment process.</p> <p></p> <p>In just a matter of minutes, the pipeline is expected to finish its run, seamlessly deploying our functions into the production environment.</p> <p></p>"},{"location":"home/multi-stage-environments/#creating-new-pipelines","title":"Creating new Pipelines","text":"<p>Feel free to create as many pipelines as you need. Just remember to instantiate them in the <code>app.py</code> file located at the root of your project.</p> app.py<pre><code>import aws_cdk as cdk\nfrom infra.stacks.dev_stack import DevStack\nfrom infra.stacks.staging_stack import StagingStack\nfrom infra.stacks.prod_stack import ProdStack\n\napp = cdk.App()\n\nDevStack(app)\nStagingStack(app)\nProdStack(app)\n\napp.synth()\n</code></pre> <p>Note</p> <p> You can deploy all pipelines at once by using the following command:  <pre><code>forge deploy --all\n</code></pre> </p>"},{"location":"home/multi-stage-environments/#overview","title":"Overview","text":"<p>By adhering to the instructions outlined in this tutorial, you are now equipped with three distinct CI/CD pipelines. Each pipeline corresponds to a specific stage of the development lifecycle, directly linked to the <code>dev</code>, <code>staging</code>, and <code>main</code> branches in your GitHub repository.</p> <p>These pipelines ensure that changes made in each branch are automatically integrated and deployed to the appropriate environment, streamlining the process from development to production.</p> <p></p> <p>Furthermore, you have deployed three unique stack of functions, each corresponding to a different environment:</p> <p></p> <ul> <li>Dev: https://zo3691q3pd.execute-api.us-east-2.amazonaws.com/dev/hello_world</li> <li>Staging: https://qkaer0f0q5.execute-api.us-east-2.amazonaws.com/staging/hello_world</li> <li>Prod: https://byi76zqidj.execute-api.us-east-2.amazonaws.com/prod/hello_world</li> </ul> <p>Each link directs you to the corresponding function deployed within its respective environment, demonstrating the successful separation and management of environments through your CI/CD workflows.</p>"},{"location":"home/securing-endpoints/","title":"Securing Endpoints Through an Authorizer","text":"<p>In this section, we will delve into securing endpoints by introducing an intermediary function known as an authorizer which will be responsible for validating incoming requests, determining if they should be allowed to access the targeted resources.</p> <p>By implementing an authorizer, you can ensure that only authenticated and authorized requests are processed by your endpoints, enhancing the security and privacy of your application.</p> <p>In fact, Lambda Forge treats all lambda functions as private by default. That's why we had to use the <code>--public</code> flag when creating the previous hello world function, to make it accessible without authentication. Without this flag, we would have been required to implement an authorizer for user authentication.</p>"},{"location":"home/securing-endpoints/#creating-an-authorizer","title":"Creating an Authorizer","text":"<p>First, let's begin by creating a new authorizer function with the following command:</p> <pre><code>forge authorizer secret --description \"An authorizer to validate requests based on a secret present on the headers\"\n</code></pre> <p>This command instructs the forge CLI tool to create a new authorizer under the <code>secret</code> directory.</p>"},{"location":"home/securing-endpoints/#authorizer-structure","title":"Authorizer Structure","text":"<p>Authorizers, while closely resembling Lambda Functions in structure, they fulfill a distinct role.</p> <p>Let's examine the structure of an authorizer more closely:</p> <pre><code>authorizers\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 secret\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <ul> <li><code>authorizers/</code> This directory serves as the central hub for all authorizer functions, analogous to how the <code>functions/</code> directory houses Lambda functions. Each distinct authorizer is allocated its own subdirectory within this folder.</li> <li><code>secret/</code> This subdirectory is specifically designed for developing the <code>secret</code> authorizer.</li> <li><code>__init__.py</code> Marks the directory as a Python package, enabling its modules to be imported elsewhere within the project.</li> <li><code>config.py</code> Contains the configuration settings for the authorizer, such as environmental variables and access control parameters.</li> <li><code>main.py</code> Houses the main logic for the authorizer, detailing how incoming requests are verified.</li> <li><code>unit.py</code> Focused on unit testing for the authorizer, these tests ensure that each part of the authorizer's code operates as expected independently.</li> <li><code>utils/</code> Provides utility functions that are used by the authorizers, offering common functionalities or resources that can be leveraged across various authorizers.</li> </ul>"},{"location":"home/securing-endpoints/#implementing-the-authorizer","title":"Implementing The Authorizer","text":"<p>Forge automatically generates a basic implementation of an AWS Lambda authorizer that checks for a secret value present on the headers to decide on granting or denying access.</p> <p>Warning</p> <p> The example below is intended solely for demonstration and learning purposes and should not be used in production environemnts. It is crucial to develop a comprehensive and secure authentication mechanism suitable for your application's security needs.</p> authorizers/secret/main.py<pre><code>def lambda_handler(event, context):\n\n    # ATTENTION: The example provided below is strictly for demonstration purposes and should NOT be deployed in a production environment.\n    # It's crucial to develop and integrate your own robust authorization mechanism tailored to your application's security requirements.\n    # To utilize the example authorizer as a temporary placeholder, ensure to include the following header in your requests:\n\n    # Header:\n    # secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8\n\n    # Remember, security is paramount. This placeholder serves as a guide to help you understand the kind of information your custom authorizer should authenticate.\n    # Please replace it with your secure, proprietary logic before going live. Happy coding!\n\n    secret = event[\"headers\"].get(\"secret\")\n\n    SECRET = \"CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8\"\n    effect = \"allow\" if secret == SECRET else \"deny\"\n\n    policy = {\n        \"policyDocument\": {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Action\": \"execute-api:Invoke\",\n                    \"Effect\": effect,\n                    \"Resource\": event[\"methodArn\"]\n                }\n            ],\n        },\n    }\n    return policy\n</code></pre> <p>The code snippet above demonstrates that the authorizer is configured to verify the presence of a header named <code>secret</code> in the request, as shown below:</p> <p><code>secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8</code></p> <p>This key serves as a simple form of authentication, granting or denying access based on its presence and accuracy in the request headers.</p> <p>The secret mentioned is automatically generated by Forge, meaning the specific secret you encounter during your implementation will differ from the example provided. Please be mindful of this distinction as you proceed.</p>"},{"location":"home/securing-endpoints/#configuring-the-authorizer","title":"Configuring The Authorizer","text":"<p>Similar to lambda functions in terms of setup, authorizers diverge in their application. Instead of establishing an endpoint on API Gateway, an authorizer is configured to control access to one or more endpoints.</p> authorizers/secret/config.py<pre><code>from infra.services import Services\n\nclass SecretAuthorizerConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"SecretAuthorizer\",\n            path=\"./authorizers/secret\",\n            description=\"An authorizer to validate requests based on a secret present on the headers\"\n        )\n\n        services.api_gateway.create_authorizer(function, name=\"secret\")\n</code></pre> <p>The configuration detailed above establishes a new authorizer, assigning it a unique identifier <code>secret</code> within the API Gateway.</p>"},{"location":"home/securing-endpoints/#adding-authorizer-to-lambda-stack","title":"Adding Authorizer To Lambda Stack","text":"<p>Similarly to the functions, an authorizer needs to be initialized within the <code>LambdaStack</code> class.</p> <p>Fortunately, Forge takes care of this automatically.</p> infra/stacks/lambda_stack.py<pre><code>from aws_cdk import Stack\nfrom constructs import Construct\nfrom infra.services import Services\nfrom lambda_forge import release\nfrom authorizers.secret.config import SecretAuthorizerConfig\nfrom functions.hello_world.config import HelloWorldConfig\n\n\n@release\nclass LambdaStack(Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n\n        super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs)\n\n        self.services = Services(self, context)\n\n        # Authorizers\n        SecretAuthorizerConfig(self.services)\n\n        # HelloWorld\n        HelloWorldConfig(self.services)\n</code></pre>"},{"location":"home/securing-endpoints/#creating-a-private-function","title":"Creating a Private Function","text":"<p>Now let's create a new private function.</p> <pre><code>forge function private --method \"GET\" --description \"A private function\"\n</code></pre> <p>Upon creating a new function using the Forge CLI, the project's function structure is expanded to include this new function alongside the existing ones.</p> <pre><code>functions\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 hello_world\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 private\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 integration.py\n    \u251c\u2500\u2500 main.py\n    \u2514\u2500\u2500 unit.py\n</code></pre>"},{"location":"home/securing-endpoints/#implementing-the-function","title":"Implementing the Function","text":"<p>This function will also output a <code>{\"message\": \"Hello World!\"}</code> message, making it identical to our existing function. The only distinction lies in its configuration.</p> functions/private/main.py<pre><code>def lambda_handler(event, context):\n\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"Hello World!\"})\n    }\n</code></pre>"},{"location":"home/securing-endpoints/#configuring-the-function-as-private","title":"Configuring the Function as Private","text":"<p>To configure the function as private, we must link it to the authorizer by passing the authorizer's name, established during its creation, to the <code>create_endpoint</code> method.</p> functions/private/config.py<pre><code>from infra.services import Services\n\nclass PrivateConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Private\",\n            path=\"./functions/private\",\n            description=\"A private function\",\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/private\", function, authorizer=\"secret\")\n</code></pre> <p>This configuration file establishes a new private function that is secured with the <code>secrets</code> authorizer and accessible via a GET request at the <code>/private</code> endpoint.</p>"},{"location":"home/securing-endpoints/#deployment-process-for-both-authorizer-and-function","title":"Deployment Process for Both Authorizer and Function","text":"<p>As the next step, let's proceed to upload our updates to GitHub.</p> <pre><code># Add all changes to the staging area\ngit add .\n\n# Commit the staged changes with a clear message\ngit commit -m \"Implemented a private function with an authorizer\"\n\n# Push the committed changes to the 'dev' branch\ngit push origin dev\n</code></pre> <p>This operation will automatically initiate our development pipeline.</p> <p></p> <p>After the pipeline completes successfully, the private Lambda function becomes operational:</p> <ul> <li>Dev: https://tbd4it3lph.execute-api.us-east-2.amazonaws.com/dev/private</li> </ul> <p>Direct access to these URLs through a web browser will display an unauthorized access message:</p> <pre><code>{\n  \"Message\": \"User is not authorized to access this resource with an explicit deny\"\n}\n</code></pre> <p>However, access is granted when including the necessary secret in the request header. Below is how to use <code>curl</code> to access the Lambda function:</p> <pre><code>curl --request GET \\\n  --url https://tbd4it3lph.execute-api.us-east-2.amazonaws.com/dev/private \\\n  --header 'secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8'\n</code></pre> <p>Upon running the curl command, you will receive the following response:</p> <pre><code>{\n  \"message\": \"Hello From Private!\"\n}\n</code></pre> <p>This validates the functionality of our authorizer, effectively securing the private Lambda function to ensure access is only available to those possessing the correct secret header.</p>"},{"location":"home/securing-endpoints/#setting-a-default-authorizer","title":"Setting a Default Authorizer","text":"<p>Lambda Forge automatically considers all functions as private unless specified otherwise. This means functions are generally expected to require an authorizer for access control, unless they are explicitly marked as public.</p> <p>To facilitate easier management and to obviate the need for specifying an authorizer for each Lambda function individually, Lambda Forge allows for the designation of a default authorizer. This default authorizer is automatically applied to all non-public Lambda functions, streamlining the configuration process for securing access.</p> <p>To set an authorizer as the default, you can use the <code>default=True</code> argument in the <code>create_authorizer</code> method when defining your authorizer.</p> authorizers/secret/config.py<pre><code>        function = services.aws_lambda.create_function(\n            name=\"SecretAuthorizer\",\n            path=\"./authorizers/secret\",\n            description=\"An authorizer to validate requests based on a secret present on the headers\"\n        )\n\n        services.api_gateway.create_authorizer(function, name=\"secret\", default=True)\n</code></pre> <p>Next, we'll update the Private Function configuration to no longer directly associate it with the <code>secrets</code> authorizer.</p> functions/private/config.py<pre><code>        function = services.aws_lambda.create_function(\n            name=\"Private\",\n            path=\"./functions/private\",\n            description=\"A private function\",\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/private\", function)\n</code></pre> <p>Having designated the <code>secret</code>authorizer as the default, any function not explicitly linked to a particular authorizer and not flagged as public, such as this one, will inherently be protected by the <code>secret</code> authorizer by default.</p>"},{"location":"license/license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2024 Guilherme Alves Pimenta</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"}]}